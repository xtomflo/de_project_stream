Attaching to de_project_stream_zookeeper_1, de_project_stream_kafka_1, de_project_stream_jobmanager_1, de_project_stream_postgres_1, de_project_stream_taskmanager_1, de_project_stream_serverloggenerator_1, de_project_stream_frauddetection_1
[32mkafka_1               |[0m waiting for kafka to be ready
[32mkafka_1               |[0m [Configuring] 'inter.broker.listener.name' in '/opt/kafka/config/server.properties'
[32mkafka_1               |[0m Excluding KAFKA_HOME from broker config
[35mpostgres_1            |[0m 
[35mpostgres_1            |[0m PostgreSQL Database directory appears to contain a database; Skipping initialization
[32mkafka_1               |[0m [Configuring] 'port' in '/opt/kafka/config/server.properties'
[35mpostgres_1            |[0m 
[35mpostgres_1            |[0m 2023-03-28 11:12:26.997 UTC [1] LOG:  starting PostgreSQL 12.2 (Debian 12.2-2.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit
[35mpostgres_1            |[0m 2023-03-28 11:12:26.998 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
[35mpostgres_1            |[0m 2023-03-28 11:12:26.998 UTC [1] LOG:  listening on IPv6 address "::", port 5432
[32mkafka_1               |[0m [Configuring] 'advertised.listeners' in '/opt/kafka/config/server.properties'
[32mkafka_1               |[0m [Configuring] 'listener.security.protocol.map' in '/opt/kafka/config/server.properties'
[35mpostgres_1            |[0m 2023-03-28 11:12:27.030 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
[35mpostgres_1            |[0m 2023-03-28 11:12:27.083 UTC [25] LOG:  database system was interrupted; last known up at 2023-03-28 11:07:14 UTC
[32mkafka_1               |[0m [Configuring] 'broker.id' in '/opt/kafka/config/server.properties'
[32mkafka_1               |[0m Excluding KAFKA_VERSION from broker config
[32mkafka_1               |[0m [Configuring] 'listeners' in '/opt/kafka/config/server.properties'
[32mkafka_1               |[0m [Configuring] 'zookeeper.connect' in '/opt/kafka/config/server.properties'
[32mkafka_1               |[0m [Configuring] 'log.dirs' in '/opt/kafka/config/server.properties'
[33;1mzookeeper_1           |[0m JMX enabled by default
[33;1mzookeeper_1           |[0m Using config: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,394 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,412 [myid:] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,412 [myid:] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 1
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,414 [myid:] - WARN  [main:QuorumPeerMain@113] - Either no config or no quorum defined in config, running  in standalone mode
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,414 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,471 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,494 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,495 [myid:] - INFO  [main:ZooKeeperServerMain@95] - Starting server
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,558 [myid:] - INFO  [main:Environment@100] - Server environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,558 [myid:] - INFO  [main:Environment@100] - Server environment:host.name=d743f975c6d5
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,559 [myid:] - INFO  [main:Environment@100] - Server environment:java.version=1.7.0_65
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,559 [myid:] - INFO  [main:Environment@100] - Server environment:java.vendor=Oracle Corporation
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,559 [myid:] - INFO  [main:Environment@100] - Server environment:java.home=/usr/lib/jvm/java-7-openjdk-amd64/jre
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,572 [myid:] - INFO  [main:Environment@100] - Server environment:java.class.path=/opt/zookeeper-3.4.6/bin/../build/classes:/opt/zookeeper-3.4.6/bin/../build/lib/*.jar:/opt/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/opt/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/opt/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/opt/zookeeper-3.4.6/bin/../lib/log4j-1.2.16.jar:/opt/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/opt/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/opt/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/opt/zookeeper-3.4.6/bin/../conf:
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,573 [myid:] - INFO  [main:Environment@100] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,573 [myid:] - INFO  [main:Environment@100] - Server environment:java.io.tmpdir=/tmp
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,577 [myid:] - INFO  [main:Environment@100] - Server environment:java.compiler=<NA>
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,578 [myid:] - INFO  [main:Environment@100] - Server environment:os.name=Linux
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,578 [myid:] - INFO  [main:Environment@100] - Server environment:os.arch=amd64
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,578 [myid:] - INFO  [main:Environment@100] - Server environment:os.version=5.4.72-microsoft-standard-WSL2
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,578 [myid:] - INFO  [main:Environment@100] - Server environment:user.name=root
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,578 [myid:] - INFO  [main:Environment@100] - Server environment:user.home=/root
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,578 [myid:] - INFO  [main:Environment@100] - Server environment:user.dir=/opt/zookeeper-3.4.6
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,580 [myid:] - INFO  [main:ZooKeeperServer@755] - tickTime set to 2000
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,581 [myid:] - INFO  [main:ZooKeeperServer@764] - minSessionTimeout set to -1
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,581 [myid:] - INFO  [main:ZooKeeperServer@773] - maxSessionTimeout set to -1
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:27,677 [myid:] - INFO  [main:NIOServerCnxnFactory@94] - binding to port 0.0.0.0/0.0.0.0:2181
[32mkafka_1               |[0m [2023-03-28 11:12:30,739] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[35mpostgres_1            |[0m 2023-03-28 11:12:31.675 UTC [25] LOG:  database system was not properly shut down; automatic recovery in progress
[35mpostgres_1            |[0m 2023-03-28 11:12:31.700 UTC [25] LOG:  invalid record length at 0/165E890: wanted 24, got 0
[35mpostgres_1            |[0m 2023-03-28 11:12:31.700 UTC [25] LOG:  redo is not required
[35mpostgres_1            |[0m 2023-03-28 11:12:31.824 UTC [1] LOG:  database system is ready to accept connections
[33mjobmanager_1          |[0m Starting standalonesession as a console application on host 75e532f08cbd.
[32mkafka_1               |[0m [2023-03-28 11:12:33,320] INFO starting (kafka.server.KafkaServer)
[32mkafka_1               |[0m [2023-03-28 11:12:33,343] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
[32mkafka_1               |[0m [2023-03-28 11:12:33,494] INFO [ZooKeeperClient] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1               |[0m [2023-03-28 11:12:33,514] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,514] INFO Client environment:host.name=d8ba92357be3 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,514] INFO Client environment:java.version=1.8.0_212 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,514] INFO Client environment:java.vendor=IcedTea (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,514] INFO Client environment:java.home=/usr/lib/jvm/java-1.8-openjdk/jre (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,514] INFO Client environment:java.class.path=/opt/kafka/bin/../libs/activation-1.1.1.jar:/opt/kafka/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/opt/kafka/bin/../libs/argparse4j-0.7.0.jar:/opt/kafka/bin/../libs/audience-annotations-0.5.0.jar:/opt/kafka/bin/../libs/commons-lang3-3.8.1.jar:/opt/kafka/bin/../libs/connect-api-2.2.1.jar:/opt/kafka/bin/../libs/connect-basic-auth-extension-2.2.1.jar:/opt/kafka/bin/../libs/connect-file-2.2.1.jar:/opt/kafka/bin/../libs/connect-json-2.2.1.jar:/opt/kafka/bin/../libs/connect-runtime-2.2.1.jar:/opt/kafka/bin/../libs/connect-transforms-2.2.1.jar:/opt/kafka/bin/../libs/guava-20.0.jar:/opt/kafka/bin/../libs/hk2-api-2.5.0-b42.jar:/opt/kafka/bin/../libs/hk2-locator-2.5.0-b42.jar:/opt/kafka/bin/../libs/hk2-utils-2.5.0-b42.jar:/opt/kafka/bin/../libs/jackson-annotations-2.9.8.jar:/opt/kafka/bin/../libs/jackson-core-2.9.8.jar:/opt/kafka/bin/../libs/jackson-databind-2.9.8.jar:/opt/kafka/bin/../libs/jackson-datatype-jdk8-2.9.8.jar:/opt/kafka/bin/../libs/jackson-jaxrs-base-2.9.8.jar:/opt/kafka/bin/../libs/jackson-jaxrs-json-provider-2.9.8.jar:/opt/kafka/bin/../libs/jackson-module-jaxb-annotations-2.9.8.jar:/opt/kafka/bin/../libs/javassist-3.22.0-CR2.jar:/opt/kafka/bin/../libs/javax.annotation-api-1.2.jar:/opt/kafka/bin/../libs/javax.inject-1.jar:/opt/kafka/bin/../libs/javax.inject-2.5.0-b42.jar:/opt/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/opt/kafka/bin/../libs/javax.ws.rs-api-2.1.1.jar:/opt/kafka/bin/../libs/javax.ws.rs-api-2.1.jar:/opt/kafka/bin/../libs/jaxb-api-2.3.0.jar:/opt/kafka/bin/../libs/jersey-client-2.27.jar:/opt/kafka/bin/../libs/jersey-common-2.27.jar:/opt/kafka/bin/../libs/jersey-container-servlet-2.27.jar:/opt/kafka/bin/../libs/jersey-container-servlet-core-2.27.jar:/opt/kafka/bin/../libs/jersey-hk2-2.27.jar:/opt/kafka/bin/../libs/jersey-media-jaxb-2.27.jar:/opt/kafka/bin/../libs/jersey-server-2.27.jar:/opt/kafka/bin/../libs/jetty-client-9.4.14.v20181114.jar:/opt/kafka/bin/../libs/jetty-continuation-9.4.14.v20181114.jar:/opt/kafka/bin/../libs/jetty-http-9.4.14.v20181114.jar:/opt/kafka/bin/../libs/jetty-io-9.4.14.v20181114.jar:/opt/kafka/bin/../libs/jetty-security-9.4.14.v20181114.jar:/opt/kafka/bin/../libs/jetty-server-9.4.14.v20181114.jar:/opt/kafka/bin/../libs/jetty-servlet-9.4.14.v20181114.jar:/opt/kafka/bin/../libs/jetty-servlets-9.4.14.v20181114.jar:/opt/kafka/bin/../libs/jetty-util-9.4.14.v20181114.jar:/opt/kafka/bin/../libs/jopt-simple-5.0.4.jar:/opt/kafka/bin/../libs/kafka-clients-2.2.1.jar:/opt/kafka/bin/../libs/kafka-log4j-appender-2.2.1.jar:/opt/kafka/bin/../libs/kafka-streams-2.2.1.jar:/opt/kafka/bin/../libs/kafka-streams-examples-2.2.1.jar:/opt/kafka/bin/../libs/kafka-streams-scala_2.12-2.2.1.jar:/opt/kafka/bin/../libs/kafka-streams-test-utils-2.2.1.jar:/opt/kafka/bin/../libs/kafka-tools-2.2.1.jar:/opt/kafka/bin/../libs/kafka_2.12-2.2.1-sources.jar:/opt/kafka/bin/../libs/kafka_2.12-2.2.1.jar:/opt/kafka/bin/../libs/log4j-1.2.17.jar:/opt/kafka/bin/../libs/lz4-java-1.5.0.jar:/opt/kafka/bin/../libs/maven-artifact-3.6.0.jar:/opt/kafka/bin/../libs/metrics-core-2.2.0.jar:/opt/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/opt/kafka/bin/../libs/plexus-utils-3.1.0.jar:/opt/kafka/bin/../libs/reflections-0.9.11.jar:/opt/kafka/bin/../libs/rocksdbjni-5.15.10.jar:/opt/kafka/bin/../libs/scala-library-2.12.8.jar:/opt/kafka/bin/../libs/scala-logging_2.12-3.9.0.jar:/opt/kafka/bin/../libs/scala-reflect-2.12.8.jar:/opt/kafka/bin/../libs/slf4j-api-1.7.25.jar:/opt/kafka/bin/../libs/slf4j-log4j12-1.7.25.jar:/opt/kafka/bin/../libs/snappy-java-1.1.7.2.jar:/opt/kafka/bin/../libs/validation-api-1.1.0.Final.jar:/opt/kafka/bin/../libs/zkclient-0.11.jar:/opt/kafka/bin/../libs/zookeeper-3.4.13.jar:/opt/kafka/bin/../libs/zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,526] INFO Client environment:java.library.path=/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,526] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,526] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,526] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,526] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,526] INFO Client environment:os.version=5.4.72-microsoft-standard-WSL2 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,526] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,527] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,527] INFO Client environment:user.dir=/ (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,529] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@4a22f9e2 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1               |[0m [2023-03-28 11:12:33,639] INFO Opening socket connection to server zookeeper/172.31.0.2:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[32mkafka_1               |[0m [2023-03-28 11:12:33,642] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:33,689 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /172.31.0.3:39494
[32mkafka_1               |[0m [2023-03-28 11:12:33,690] INFO Socket connection established to zookeeper/172.31.0.2:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:33,735 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /172.31.0.3:39494
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:33,738 [myid:] - INFO  [SyncThread:0:FileTxnLog@199] - Creating new log file: log.1
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:33,897 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x18727eb58ad0000 with negotiated timeout 6000 for client /172.31.0.3:39494
[32mkafka_1               |[0m [2023-03-28 11:12:33,900] INFO Session establishment complete on server zookeeper/172.31.0.2:2181, sessionid = 0x18727eb58ad0000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[32mkafka_1               |[0m [2023-03-28 11:12:33,920] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:34,369 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x18727eb58ad0000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:34,444 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x18727eb58ad0000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:34,471 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x18727eb58ad0000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
[36;1mtaskmanager_1         |[0m Starting taskexecutor as a console application on host a189f5323dec.
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:35,316 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x18727eb58ad0000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
[32mkafka_1               |[0m [2023-03-28 11:12:35,351] INFO Cluster ID = K-fF8chpRfC_DShTFbvwhw (kafka.server.KafkaServer)
[32mkafka_1               |[0m [2023-03-28 11:12:35,364] WARN No meta.properties file under dir /kafka/kafka-logs-d8ba92357be3/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,682 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,716 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Preconfiguration: 
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,716 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - 
[33mjobmanager_1          |[0m 
[33mjobmanager_1          |[0m 
[33mjobmanager_1          |[0m RESOURCE_PARAMS extraction logs:
[33mjobmanager_1          |[0m jvm_params: -Xmx1073741824 -Xms1073741824 -XX:MaxMetaspaceSize=268435456
[33mjobmanager_1          |[0m dynamic_configs: -D jobmanager.memory.off-heap.size=134217728b -D jobmanager.memory.jvm-overhead.min=201326592b -D jobmanager.memory.jvm-metaspace.size=268435456b -D jobmanager.memory.heap.size=1073741824b -D jobmanager.memory.jvm-overhead.max=201326592b
[33mjobmanager_1          |[0m logs: INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1600Mb
[33mjobmanager_1          |[0m INFO  [] - Loading configuration property: jobmanager.rpc.address, jobmanager
[33mjobmanager_1          |[0m INFO  [] - Loading configuration property: blob.server.port, 6124
[33mjobmanager_1          |[0m INFO  [] - Loading configuration property: query.server.port, 6125
[33mjobmanager_1          |[0m INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1728Mb
[33mjobmanager_1          |[0m INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
[33mjobmanager_1          |[0m INFO  [] - Loading configuration property: state.backend, filesystem
[33mjobmanager_1          |[0m INFO  [] - Loading configuration property: state.checkpoints.dir, file:///tmp/flink-checkpoints-directory
[33mjobmanager_1          |[0m INFO  [] - Loading configuration property: state.savepoints.dir, file:///tmp/flink-savepoints-directory
[33mjobmanager_1          |[0m INFO  [] - Loading configuration property: heartbeat.interval, 1000
[33mjobmanager_1          |[0m INFO  [] - Loading configuration property: heartbeat.timeout, 5000
[33mjobmanager_1          |[0m INFO  [] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
[33mjobmanager_1          |[0m INFO  [] - Final Master Memory configuration:
[33mjobmanager_1          |[0m INFO  [] -   Total Process Memory: 1.563gb (1677721600 bytes)
[33mjobmanager_1          |[0m INFO  [] -     Total Flink Memory: 1.125gb (1207959552 bytes)
[33mjobmanager_1          |[0m INFO  [] -       JVM Heap:         1024.000mb (1073741824 bytes)
[33mjobmanager_1          |[0m INFO  [] -       Off-heap:         128.000mb (134217728 bytes)
[33mjobmanager_1          |[0m INFO  [] -     JVM Metaspace:      256.000mb (268435456 bytes)
[33mjobmanager_1          |[0m INFO  [] -     JVM Overhead:       192.000mb (201326592 bytes)
[33mjobmanager_1          |[0m 
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,717 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,717 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Starting StandaloneSessionClusterEntrypoint (Version: 1.14.6, Scala: 2.12, Rev:a921a4d, Date:2022-09-09T10:18:38+02:00)
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,717 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS current user: flink
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,718 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current Hadoop/Kerberos user: <no hadoop dependency found>
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,718 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM: OpenJDK 64-Bit Server VM - Temurin - 1.8/25.345-b01
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,718 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum heap size: 981 MiBytes
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,718 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JAVA_HOME: /opt/java/openjdk
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,718 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  No Hadoop Dependency available
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,718 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM Options:
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,719 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xmx1073741824
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,719 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xms1073741824
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,719 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:MaxMetaspaceSize=268435456
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,719 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog.file=/opt/flink/log/flink--standalonesession-1-75e532f08cbd.log
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,719 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,719 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,727 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,727 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program Arguments:
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,729 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --configDir
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,729 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     /opt/flink/conf
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,734 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     --executionMode
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,735 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     cluster
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,735 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,736 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.off-heap.size=134217728b
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,736 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,736 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.min=201326592b
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,736 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,736 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-metaspace.size=268435456b
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,736 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,737 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.heap.size=1073741824b
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,749 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,750 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.max=201326592b
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,750 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Classpath: /opt/flink/lib/flink-csv-1.14.6.jar:/opt/flink/lib/flink-json-1.14.6.jar:/opt/flink/lib/flink-shaded-zookeeper-3.4.14.jar:/opt/flink/lib/flink-table_2.12-1.14.6.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/flink-dist_2.12-1.14.6.jar:::
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,751 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,754 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Registered UNIX signal handlers for [TERM, HUP, INT]
[32mkafka_1               |[0m [2023-03-28 11:12:35,752] INFO KafkaConfig values: 
[32mkafka_1               |[0m 	advertised.host.name = null
[32mkafka_1               |[0m 	advertised.listeners = INSIDE://:9092,OUTSIDE://:9094
[32mkafka_1               |[0m 	advertised.port = null
[32mkafka_1               |[0m 	alter.config.policy.class.name = null
[32mkafka_1               |[0m 	alter.log.dirs.replication.quota.window.num = 11
[32mkafka_1               |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[32mkafka_1               |[0m 	authorizer.class.name = 
[32mkafka_1               |[0m 	auto.create.topics.enable = true
[32mkafka_1               |[0m 	auto.leader.rebalance.enable = true
[32mkafka_1               |[0m 	background.threads = 10
[32mkafka_1               |[0m 	broker.id = -1
[32mkafka_1               |[0m 	broker.id.generation.enable = true
[32mkafka_1               |[0m 	broker.rack = null
[32mkafka_1               |[0m 	client.quota.callback.class = null
[32mkafka_1               |[0m 	compression.type = producer
[32mkafka_1               |[0m 	connection.failed.authentication.delay.ms = 100
[32mkafka_1               |[0m 	connections.max.idle.ms = 600000
[32mkafka_1               |[0m 	connections.max.reauth.ms = 0
[32mkafka_1               |[0m 	control.plane.listener.name = null
[32mkafka_1               |[0m 	controlled.shutdown.enable = true
[32mkafka_1               |[0m 	controlled.shutdown.max.retries = 3
[32mkafka_1               |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[32mkafka_1               |[0m 	controller.socket.timeout.ms = 30000
[32mkafka_1               |[0m 	create.topic.policy.class.name = null
[32mkafka_1               |[0m 	default.replication.factor = 1
[32mkafka_1               |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[32mkafka_1               |[0m 	delegation.token.expiry.time.ms = 86400000
[32mkafka_1               |[0m 	delegation.token.master.key = null
[32mkafka_1               |[0m 	delegation.token.max.lifetime.ms = 604800000
[32mkafka_1               |[0m 	delete.records.purgatory.purge.interval.requests = 1
[32mkafka_1               |[0m 	delete.topic.enable = true
[32mkafka_1               |[0m 	fetch.purgatory.purge.interval.requests = 1000
[32mkafka_1               |[0m 	group.initial.rebalance.delay.ms = 0
[32mkafka_1               |[0m 	group.max.session.timeout.ms = 300000
[32mkafka_1               |[0m 	group.max.size = 2147483647
[32mkafka_1               |[0m 	group.min.session.timeout.ms = 6000
[32mkafka_1               |[0m 	host.name = 
[32mkafka_1               |[0m 	inter.broker.listener.name = INSIDE
[32mkafka_1               |[0m 	inter.broker.protocol.version = 2.2-IV1
[32mkafka_1               |[0m 	kafka.metrics.polling.interval.secs = 10
[32mkafka_1               |[0m 	kafka.metrics.reporters = []
[32mkafka_1               |[0m 	leader.imbalance.check.interval.seconds = 300
[32mkafka_1               |[0m 	leader.imbalance.per.broker.percentage = 10
[32mkafka_1               |[0m 	listener.security.protocol.map = INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
[32mkafka_1               |[0m 	listeners = INSIDE://:9092,OUTSIDE://:9094
[32mkafka_1               |[0m 	log.cleaner.backoff.ms = 15000
[32mkafka_1               |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[32mkafka_1               |[0m 	log.cleaner.delete.retention.ms = 86400000
[32mkafka_1               |[0m 	log.cleaner.enable = true
[32mkafka_1               |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[32mkafka_1               |[0m 	log.cleaner.io.buffer.size = 524288
[32mkafka_1               |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[32mkafka_1               |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[32mkafka_1               |[0m 	log.cleaner.min.compaction.lag.ms = 0
[32mkafka_1               |[0m 	log.cleaner.threads = 1
[32mkafka_1               |[0m 	log.cleanup.policy = [delete]
[32mkafka_1               |[0m 	log.dir = /tmp/kafka-logs
[32mkafka_1               |[0m 	log.dirs = /kafka/kafka-logs-d8ba92357be3
[32mkafka_1               |[0m 	log.flush.interval.messages = 9223372036854775807
[32mkafka_1               |[0m 	log.flush.interval.ms = null
[32mkafka_1               |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[32mkafka_1               |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[32mkafka_1               |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[32mkafka_1               |[0m 	log.index.interval.bytes = 4096
[32mkafka_1               |[0m 	log.index.size.max.bytes = 10485760
[32mkafka_1               |[0m 	log.message.downconversion.enable = true
[32mkafka_1               |[0m 	log.message.format.version = 2.2-IV1
[32mkafka_1               |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[32mkafka_1               |[0m 	log.message.timestamp.type = CreateTime
[32mkafka_1               |[0m 	log.preallocate = false
[32mkafka_1               |[0m 	log.retention.bytes = -1
[32mkafka_1               |[0m 	log.retention.check.interval.ms = 300000
[32mkafka_1               |[0m 	log.retention.hours = 168
[32mkafka_1               |[0m 	log.retention.minutes = null
[32mkafka_1               |[0m 	log.retention.ms = null
[32mkafka_1               |[0m 	log.roll.hours = 168
[32mkafka_1               |[0m 	log.roll.jitter.hours = 0
[32mkafka_1               |[0m 	log.roll.jitter.ms = null
[32mkafka_1               |[0m 	log.roll.ms = null
[32mkafka_1               |[0m 	log.segment.bytes = 1073741824
[32mkafka_1               |[0m 	log.segment.delete.delay.ms = 60000
[32mkafka_1               |[0m 	max.connections.per.ip = 2147483647
[32mkafka_1               |[0m 	max.connections.per.ip.overrides = 
[32mkafka_1               |[0m 	max.incremental.fetch.session.cache.slots = 1000
[32mkafka_1               |[0m 	message.max.bytes = 1000012
[32mkafka_1               |[0m 	metric.reporters = []
[32mkafka_1               |[0m 	metrics.num.samples = 2
[32mkafka_1               |[0m 	metrics.recording.level = INFO
[32mkafka_1               |[0m 	metrics.sample.window.ms = 30000
[32mkafka_1               |[0m 	min.insync.replicas = 1
[32mkafka_1               |[0m 	num.io.threads = 8
[32mkafka_1               |[0m 	num.network.threads = 3
[32mkafka_1               |[0m 	num.partitions = 1
[32mkafka_1               |[0m 	num.recovery.threads.per.data.dir = 1
[32mkafka_1               |[0m 	num.replica.alter.log.dirs.threads = null
[32mkafka_1               |[0m 	num.replica.fetchers = 1
[32mkafka_1               |[0m 	offset.metadata.max.bytes = 4096
[32mkafka_1               |[0m 	offsets.commit.required.acks = -1
[32mkafka_1               |[0m 	offsets.commit.timeout.ms = 5000
[32mkafka_1               |[0m 	offsets.load.buffer.size = 5242880
[32mkafka_1               |[0m 	offsets.retention.check.interval.ms = 600000
[32mkafka_1               |[0m 	offsets.retention.minutes = 10080
[32mkafka_1               |[0m 	offsets.topic.compression.codec = 0
[32mkafka_1               |[0m 	offsets.topic.num.partitions = 50
[32mkafka_1               |[0m 	offsets.topic.replication.factor = 1
[32mkafka_1               |[0m 	offsets.topic.segment.bytes = 104857600
[32mkafka_1               |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[32mkafka_1               |[0m 	password.encoder.iterations = 4096
[32mkafka_1               |[0m 	password.encoder.key.length = 128
[32mkafka_1               |[0m 	password.encoder.keyfactory.algorithm = null
[32mkafka_1               |[0m 	password.encoder.old.secret = null
[32mkafka_1               |[0m 	password.encoder.secret = null
[32mkafka_1               |[0m 	port = 9092
[32mkafka_1               |[0m 	principal.builder.class = null
[32mkafka_1               |[0m 	producer.purgatory.purge.interval.requests = 1000
[32mkafka_1               |[0m 	queued.max.request.bytes = -1
[32mkafka_1               |[0m 	queued.max.requests = 500
[32mkafka_1               |[0m 	quota.consumer.default = 9223372036854775807
[32mkafka_1               |[0m 	quota.producer.default = 9223372036854775807
[32mkafka_1               |[0m 	quota.window.num = 11
[32mkafka_1               |[0m 	quota.window.size.seconds = 1
[32mkafka_1               |[0m 	replica.fetch.backoff.ms = 1000
[32mkafka_1               |[0m 	replica.fetch.max.bytes = 1048576
[32mkafka_1               |[0m 	replica.fetch.min.bytes = 1
[32mkafka_1               |[0m 	replica.fetch.response.max.bytes = 10485760
[32mkafka_1               |[0m 	replica.fetch.wait.max.ms = 500
[32mkafka_1               |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[32mkafka_1               |[0m 	replica.lag.time.max.ms = 10000
[32mkafka_1               |[0m 	replica.socket.receive.buffer.bytes = 65536
[32mkafka_1               |[0m 	replica.socket.timeout.ms = 30000
[32mkafka_1               |[0m 	replication.quota.window.num = 11
[32mkafka_1               |[0m 	replication.quota.window.size.seconds = 1
[32mkafka_1               |[0m 	request.timeout.ms = 30000
[32mkafka_1               |[0m 	reserved.broker.max.id = 1000
[32mkafka_1               |[0m 	sasl.client.callback.handler.class = null
[32mkafka_1               |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[32mkafka_1               |[0m 	sasl.jaas.config = null
[32mkafka_1               |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[32mkafka_1               |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[32mkafka_1               |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[32mkafka_1               |[0m 	sasl.kerberos.service.name = null
[32mkafka_1               |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[32mkafka_1               |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[32mkafka_1               |[0m 	sasl.login.callback.handler.class = null
[32mkafka_1               |[0m 	sasl.login.class = null
[32mkafka_1               |[0m 	sasl.login.refresh.buffer.seconds = 300
[32mkafka_1               |[0m 	sasl.login.refresh.min.period.seconds = 60
[32mkafka_1               |[0m 	sasl.login.refresh.window.factor = 0.8
[32mkafka_1               |[0m 	sasl.login.refresh.window.jitter = 0.05
[32mkafka_1               |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[32mkafka_1               |[0m 	sasl.server.callback.handler.class = null
[32mkafka_1               |[0m 	security.inter.broker.protocol = PLAINTEXT
[32mkafka_1               |[0m 	socket.receive.buffer.bytes = 102400
[32mkafka_1               |[0m 	socket.request.max.bytes = 104857600
[32mkafka_1               |[0m 	socket.send.buffer.bytes = 102400
[32mkafka_1               |[0m 	ssl.cipher.suites = []
[32mkafka_1               |[0m 	ssl.client.auth = none
[32mkafka_1               |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[32mkafka_1               |[0m 	ssl.endpoint.identification.algorithm = https
[32mkafka_1               |[0m 	ssl.key.password = null
[32mkafka_1               |[0m 	ssl.keymanager.algorithm = SunX509
[32mkafka_1               |[0m 	ssl.keystore.location = null
[32mkafka_1               |[0m 	ssl.keystore.password = null
[32mkafka_1               |[0m 	ssl.keystore.type = JKS
[32mkafka_1               |[0m 	ssl.principal.mapping.rules = [DEFAULT]
[32mkafka_1               |[0m 	ssl.protocol = TLS
[32mkafka_1               |[0m 	ssl.provider = null
[32mkafka_1               |[0m 	ssl.secure.random.implementation = null
[32mkafka_1               |[0m 	ssl.trustmanager.algorithm = PKIX
[32mkafka_1               |[0m 	ssl.truststore.location = null
[32mkafka_1               |[0m 	ssl.truststore.password = null
[32mkafka_1               |[0m 	ssl.truststore.type = JKS
[32mkafka_1               |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[32mkafka_1               |[0m 	transaction.max.timeout.ms = 900000
[32mkafka_1               |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[32mkafka_1               |[0m 	transaction.state.log.load.buffer.size = 5242880
[32mkafka_1               |[0m 	transaction.state.log.min.isr = 1
[32mkafka_1               |[0m 	transaction.state.log.num.partitions = 50
[32mkafka_1               |[0m 	transaction.state.log.replication.factor = 1
[32mkafka_1               |[0m 	transaction.state.log.segment.bytes = 104857600
[32mkafka_1               |[0m 	transactional.id.expiration.ms = 604800000
[32mkafka_1               |[0m 	unclean.leader.election.enable = false
[32mkafka_1               |[0m 	zookeeper.connect = zookeeper:2181
[32mkafka_1               |[0m 	zookeeper.connection.timeout.ms = 6000
[32mkafka_1               |[0m 	zookeeper.max.in.flight.requests = 10
[32mkafka_1               |[0m 	zookeeper.session.timeout.ms = 6000
[32mkafka_1               |[0m 	zookeeper.set.acl = false
[32mkafka_1               |[0m 	zookeeper.sync.time.ms = 2000
[32mkafka_1               |[0m  (kafka.server.KafkaConfig)
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,824 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600Mb
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,829 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, jobmanager
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,831 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: blob.server.port, 6124
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,831 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: query.server.port, 6125
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,832 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728Mb
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,833 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,836 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend, filesystem
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,839 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.checkpoints.dir, file:///tmp/flink-checkpoints-directory
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,840 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.savepoints.dir, file:///tmp/flink-savepoints-directory
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,842 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: heartbeat.interval, 1000
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,843 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: heartbeat.timeout, 5000
[32mkafka_1               |[0m [2023-03-28 11:12:35,858] INFO KafkaConfig values: 
[32mkafka_1               |[0m 	advertised.host.name = null
[32mkafka_1               |[0m 	advertised.listeners = INSIDE://:9092,OUTSIDE://:9094
[32mkafka_1               |[0m 	advertised.port = null
[32mkafka_1               |[0m 	alter.config.policy.class.name = null
[32mkafka_1               |[0m 	alter.log.dirs.replication.quota.window.num = 11
[32mkafka_1               |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[32mkafka_1               |[0m 	authorizer.class.name = 
[32mkafka_1               |[0m 	auto.create.topics.enable = true
[32mkafka_1               |[0m 	auto.leader.rebalance.enable = true
[32mkafka_1               |[0m 	background.threads = 10
[32mkafka_1               |[0m 	broker.id = -1
[32mkafka_1               |[0m 	broker.id.generation.enable = true
[32mkafka_1               |[0m 	broker.rack = null
[32mkafka_1               |[0m 	client.quota.callback.class = null
[32mkafka_1               |[0m 	compression.type = producer
[32mkafka_1               |[0m 	connection.failed.authentication.delay.ms = 100
[32mkafka_1               |[0m 	connections.max.idle.ms = 600000
[32mkafka_1               |[0m 	connections.max.reauth.ms = 0
[32mkafka_1               |[0m 	control.plane.listener.name = null
[32mkafka_1               |[0m 	controlled.shutdown.enable = true
[32mkafka_1               |[0m 	controlled.shutdown.max.retries = 3
[32mkafka_1               |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[32mkafka_1               |[0m 	controller.socket.timeout.ms = 30000
[32mkafka_1               |[0m 	create.topic.policy.class.name = null
[32mkafka_1               |[0m 	default.replication.factor = 1
[32mkafka_1               |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[32mkafka_1               |[0m 	delegation.token.expiry.time.ms = 86400000
[32mkafka_1               |[0m 	delegation.token.master.key = null
[32mkafka_1               |[0m 	delegation.token.max.lifetime.ms = 604800000
[32mkafka_1               |[0m 	delete.records.purgatory.purge.interval.requests = 1
[32mkafka_1               |[0m 	delete.topic.enable = true
[32mkafka_1               |[0m 	fetch.purgatory.purge.interval.requests = 1000
[32mkafka_1               |[0m 	group.initial.rebalance.delay.ms = 0
[32mkafka_1               |[0m 	group.max.session.timeout.ms = 300000
[32mkafka_1               |[0m 	group.max.size = 2147483647
[32mkafka_1               |[0m 	group.min.session.timeout.ms = 6000
[32mkafka_1               |[0m 	host.name = 
[32mkafka_1               |[0m 	inter.broker.listener.name = INSIDE
[32mkafka_1               |[0m 	inter.broker.protocol.version = 2.2-IV1
[32mkafka_1               |[0m 	kafka.metrics.polling.interval.secs = 10
[32mkafka_1               |[0m 	kafka.metrics.reporters = []
[32mkafka_1               |[0m 	leader.imbalance.check.interval.seconds = 300
[32mkafka_1               |[0m 	leader.imbalance.per.broker.percentage = 10
[32mkafka_1               |[0m 	listener.security.protocol.map = INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
[32mkafka_1               |[0m 	listeners = INSIDE://:9092,OUTSIDE://:9094
[32mkafka_1               |[0m 	log.cleaner.backoff.ms = 15000
[32mkafka_1               |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[32mkafka_1               |[0m 	log.cleaner.delete.retention.ms = 86400000
[32mkafka_1               |[0m 	log.cleaner.enable = true
[32mkafka_1               |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[32mkafka_1               |[0m 	log.cleaner.io.buffer.size = 524288
[32mkafka_1               |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[32mkafka_1               |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[32mkafka_1               |[0m 	log.cleaner.min.compaction.lag.ms = 0
[32mkafka_1               |[0m 	log.cleaner.threads = 1
[32mkafka_1               |[0m 	log.cleanup.policy = [delete]
[32mkafka_1               |[0m 	log.dir = /tmp/kafka-logs
[32mkafka_1               |[0m 	log.dirs = /kafka/kafka-logs-d8ba92357be3
[32mkafka_1               |[0m 	log.flush.interval.messages = 9223372036854775807
[32mkafka_1               |[0m 	log.flush.interval.ms = null
[32mkafka_1               |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[32mkafka_1               |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[32mkafka_1               |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[32mkafka_1               |[0m 	log.index.interval.bytes = 4096
[32mkafka_1               |[0m 	log.index.size.max.bytes = 10485760
[32mkafka_1               |[0m 	log.message.downconversion.enable = true
[32mkafka_1               |[0m 	log.message.format.version = 2.2-IV1
[32mkafka_1               |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[32mkafka_1               |[0m 	log.message.timestamp.type = CreateTime
[32mkafka_1               |[0m 	log.preallocate = false
[32mkafka_1               |[0m 	log.retention.bytes = -1
[32mkafka_1               |[0m 	log.retention.check.interval.ms = 300000
[32mkafka_1               |[0m 	log.retention.hours = 168
[32mkafka_1               |[0m 	log.retention.minutes = null
[32mkafka_1               |[0m 	log.retention.ms = null
[32mkafka_1               |[0m 	log.roll.hours = 168
[32mkafka_1               |[0m 	log.roll.jitter.hours = 0
[32mkafka_1               |[0m 	log.roll.jitter.ms = null
[32mkafka_1               |[0m 	log.roll.ms = null
[32mkafka_1               |[0m 	log.segment.bytes = 1073741824
[32mkafka_1               |[0m 	log.segment.delete.delay.ms = 60000
[32mkafka_1               |[0m 	max.connections.per.ip = 2147483647
[32mkafka_1               |[0m 	max.connections.per.ip.overrides = 
[32mkafka_1               |[0m 	max.incremental.fetch.session.cache.slots = 1000
[32mkafka_1               |[0m 	message.max.bytes = 1000012
[32mkafka_1               |[0m 	metric.reporters = []
[32mkafka_1               |[0m 	metrics.num.samples = 2
[32mkafka_1               |[0m 	metrics.recording.level = INFO
[32mkafka_1               |[0m 	metrics.sample.window.ms = 30000
[32mkafka_1               |[0m 	min.insync.replicas = 1
[32mkafka_1               |[0m 	num.io.threads = 8
[32mkafka_1               |[0m 	num.network.threads = 3
[32mkafka_1               |[0m 	num.partitions = 1
[32mkafka_1               |[0m 	num.recovery.threads.per.data.dir = 1
[32mkafka_1               |[0m 	num.replica.alter.log.dirs.threads = null
[32mkafka_1               |[0m 	num.replica.fetchers = 1
[32mkafka_1               |[0m 	offset.metadata.max.bytes = 4096
[32mkafka_1               |[0m 	offsets.commit.required.acks = -1
[32mkafka_1               |[0m 	offsets.commit.timeout.ms = 5000
[32mkafka_1               |[0m 	offsets.load.buffer.size = 5242880
[32mkafka_1               |[0m 	offsets.retention.check.interval.ms = 600000
[32mkafka_1               |[0m 	offsets.retention.minutes = 10080
[32mkafka_1               |[0m 	offsets.topic.compression.codec = 0
[32mkafka_1               |[0m 	offsets.topic.num.partitions = 50
[32mkafka_1               |[0m 	offsets.topic.replication.factor = 1
[32mkafka_1               |[0m 	offsets.topic.segment.bytes = 104857600
[32mkafka_1               |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[32mkafka_1               |[0m 	password.encoder.iterations = 4096
[32mkafka_1               |[0m 	password.encoder.key.length = 128
[32mkafka_1               |[0m 	password.encoder.keyfactory.algorithm = null
[32mkafka_1               |[0m 	password.encoder.old.secret = null
[32mkafka_1               |[0m 	password.encoder.secret = null
[32mkafka_1               |[0m 	port = 9092
[32mkafka_1               |[0m 	principal.builder.class = null
[32mkafka_1               |[0m 	producer.purgatory.purge.interval.requests = 1000
[32mkafka_1               |[0m 	queued.max.request.bytes = -1
[32mkafka_1               |[0m 	queued.max.requests = 500
[32mkafka_1               |[0m 	quota.consumer.default = 9223372036854775807
[32mkafka_1               |[0m 	quota.producer.default = 9223372036854775807
[32mkafka_1               |[0m 	quota.window.num = 11
[32mkafka_1               |[0m 	quota.window.size.seconds = 1
[32mkafka_1               |[0m 	replica.fetch.backoff.ms = 1000
[32mkafka_1               |[0m 	replica.fetch.max.bytes = 1048576
[32mkafka_1               |[0m 	replica.fetch.min.bytes = 1
[32mkafka_1               |[0m 	replica.fetch.response.max.bytes = 10485760
[32mkafka_1               |[0m 	replica.fetch.wait.max.ms = 500
[32mkafka_1               |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[32mkafka_1               |[0m 	replica.lag.time.max.ms = 10000
[32mkafka_1               |[0m 	replica.socket.receive.buffer.bytes = 65536
[32mkafka_1               |[0m 	replica.socket.timeout.ms = 30000
[32mkafka_1               |[0m 	replication.quota.window.num = 11
[32mkafka_1               |[0m 	replication.quota.window.size.seconds = 1
[32mkafka_1               |[0m 	request.timeout.ms = 30000
[32mkafka_1               |[0m 	reserved.broker.max.id = 1000
[32mkafka_1               |[0m 	sasl.client.callback.handler.class = null
[32mkafka_1               |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[32mkafka_1               |[0m 	sasl.jaas.config = null
[32mkafka_1               |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[32mkafka_1               |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[32mkafka_1               |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[32mkafka_1               |[0m 	sasl.kerberos.service.name = null
[32mkafka_1               |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[32mkafka_1               |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[32mkafka_1               |[0m 	sasl.login.callback.handler.class = null
[32mkafka_1               |[0m 	sasl.login.class = null
[32mkafka_1               |[0m 	sasl.login.refresh.buffer.seconds = 300
[32mkafka_1               |[0m 	sasl.login.refresh.min.period.seconds = 60
[32mkafka_1               |[0m 	sasl.login.refresh.window.factor = 0.8
[32mkafka_1               |[0m 	sasl.login.refresh.window.jitter = 0.05
[32mkafka_1               |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[32mkafka_1               |[0m 	sasl.server.callback.handler.class = null
[32mkafka_1               |[0m 	security.inter.broker.protocol = PLAINTEXT
[32mkafka_1               |[0m 	socket.receive.buffer.bytes = 102400
[32mkafka_1               |[0m 	socket.request.max.bytes = 104857600
[32mkafka_1               |[0m 	socket.send.buffer.bytes = 102400
[32mkafka_1               |[0m 	ssl.cipher.suites = []
[32mkafka_1               |[0m 	ssl.client.auth = none
[32mkafka_1               |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[32mkafka_1               |[0m 	ssl.endpoint.identification.algorithm = https
[32mkafka_1               |[0m 	ssl.key.password = null
[32mkafka_1               |[0m 	ssl.keymanager.algorithm = SunX509
[32mkafka_1               |[0m 	ssl.keystore.location = null
[32mkafka_1               |[0m 	ssl.keystore.password = null
[32mkafka_1               |[0m 	ssl.keystore.type = JKS
[32mkafka_1               |[0m 	ssl.principal.mapping.rules = [DEFAULT]
[32mkafka_1               |[0m 	ssl.protocol = TLS
[32mkafka_1               |[0m 	ssl.provider = null
[32mkafka_1               |[0m 	ssl.secure.random.implementation = null
[32mkafka_1               |[0m 	ssl.trustmanager.algorithm = PKIX
[32mkafka_1               |[0m 	ssl.truststore.location = null
[32mkafka_1               |[0m 	ssl.truststore.password = null
[32mkafka_1               |[0m 	ssl.truststore.type = JKS
[32mkafka_1               |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[32mkafka_1               |[0m 	transaction.max.timeout.ms = 900000
[32mkafka_1               |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[32mkafka_1               |[0m 	transaction.state.log.load.buffer.size = 5242880
[32mkafka_1               |[0m 	transaction.state.log.min.isr = 1
[32mkafka_1               |[0m 	transaction.state.log.num.partitions = 50
[32mkafka_1               |[0m 	transaction.state.log.replication.factor = 1
[32mkafka_1               |[0m 	transaction.state.log.segment.bytes = 104857600
[32mkafka_1               |[0m 	transactional.id.expiration.ms = 604800000
[32mkafka_1               |[0m 	unclean.leader.election.enable = false
[32mkafka_1               |[0m 	zookeeper.connect = zookeeper:2181
[32mkafka_1               |[0m 	zookeeper.connection.timeout.ms = 6000
[32mkafka_1               |[0m 	zookeeper.max.in.flight.requests = 10
[32mkafka_1               |[0m 	zookeeper.session.timeout.ms = 6000
[32mkafka_1               |[0m 	zookeeper.set.acl = false
[32mkafka_1               |[0m 	zookeeper.sync.time.ms = 2000
[32mkafka_1               |[0m  (kafka.server.KafkaConfig)
[33mjobmanager_1          |[0m 2023-03-28 11:12:35,967 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting StandaloneSessionClusterEntrypoint.
[32mkafka_1               |[0m [2023-03-28 11:12:36,046] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1               |[0m [2023-03-28 11:12:36,079] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1               |[0m [2023-03-28 11:12:36,047] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1               |[0m [2023-03-28 11:12:36,197] INFO Log directory /kafka/kafka-logs-d8ba92357be3 not found, creating it. (kafka.log.LogManager)
[33mjobmanager_1          |[0m 2023-03-28 11:12:36,201 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install default filesystem.
[33mjobmanager_1          |[0m 2023-03-28 11:12:36,206 INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
[32mkafka_1               |[0m [2023-03-28 11:12:36,216] INFO Loading logs. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:12:36,249] INFO Logs loading complete in 32 ms. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:12:36,286] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:12:36,315] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[33mjobmanager_1          |[0m 2023-03-28 11:12:36,331 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install security context.
[33mjobmanager_1          |[0m 2023-03-28 11:12:36,440 INFO  org.apache.flink.runtime.security.modules.HadoopModuleFactory [] - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.
[33mjobmanager_1          |[0m 2023-03-28 11:12:36,446 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-2962112757701169931.conf.
[33mjobmanager_1          |[0m 2023-03-28 11:12:36,457 INFO  org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory [] - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.
[33mjobmanager_1          |[0m 2023-03-28 11:12:36,466 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Initializing cluster services.
[32mkafka_1               |[0m waiting for kafka to be ready
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,445 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,466 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Preconfiguration: 
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,466 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - 
[36;1mtaskmanager_1         |[0m 
[36;1mtaskmanager_1         |[0m 
[36;1mtaskmanager_1         |[0m RESOURCE_PARAMS extraction logs:
[36;1mtaskmanager_1         |[0m jvm_params: -Xmx536870902 -Xms536870902 -XX:MaxDirectMemorySize=268435458 -XX:MaxMetaspaceSize=268435456
[36;1mtaskmanager_1         |[0m dynamic_configs: -D taskmanager.memory.network.min=134217730b -D taskmanager.cpu.cores=1.0 -D taskmanager.memory.task.off-heap.size=0b -D taskmanager.memory.jvm-metaspace.size=268435456b -D external-resources=none -D taskmanager.memory.jvm-overhead.min=201326592b -D taskmanager.memory.framework.off-heap.size=134217728b -D taskmanager.memory.network.max=134217730b -D taskmanager.memory.framework.heap.size=134217728b -D taskmanager.memory.managed.size=536870920b -D taskmanager.memory.task.heap.size=402653174b -D taskmanager.numberOfTaskSlots=1 -D taskmanager.memory.jvm-overhead.max=201326592b
[36;1mtaskmanager_1         |[0m logs: INFO  [] - Loading configuration property: jobmanager.memory.process.size, 1600Mb
[36;1mtaskmanager_1         |[0m INFO  [] - Loading configuration property: jobmanager.rpc.address, jobmanager
[36;1mtaskmanager_1         |[0m INFO  [] - Loading configuration property: blob.server.port, 6124
[36;1mtaskmanager_1         |[0m INFO  [] - Loading configuration property: query.server.port, 6125
[36;1mtaskmanager_1         |[0m INFO  [] - Loading configuration property: taskmanager.memory.process.size, 1728Mb
[36;1mtaskmanager_1         |[0m INFO  [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
[36;1mtaskmanager_1         |[0m INFO  [] - Loading configuration property: state.backend, filesystem
[36;1mtaskmanager_1         |[0m INFO  [] - Loading configuration property: state.checkpoints.dir, file:///tmp/flink-checkpoints-directory
[36;1mtaskmanager_1         |[0m INFO  [] - Loading configuration property: state.savepoints.dir, file:///tmp/flink-savepoints-directory
[36;1mtaskmanager_1         |[0m INFO  [] - Loading configuration property: heartbeat.interval, 1000
[36;1mtaskmanager_1         |[0m INFO  [] - Loading configuration property: heartbeat.timeout, 5000
[36;1mtaskmanager_1         |[0m INFO  [] - The derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
[36;1mtaskmanager_1         |[0m INFO  [] - Final TaskExecutor Memory configuration:
[36;1mtaskmanager_1         |[0m INFO  [] -   Total Process Memory:          1.688gb (1811939328 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -     Total Flink Memory:          1.250gb (1342177280 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -       Total JVM Heap Memory:     512.000mb (536870902 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -         Framework:               128.000mb (134217728 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -         Task:                    384.000mb (402653174 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -       Total Off-heap Memory:     768.000mb (805306378 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -         Managed:                 512.000mb (536870920 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -         Total JVM Direct Memory: 256.000mb (268435458 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -           Framework:             128.000mb (134217728 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -           Task:                  0 bytes
[36;1mtaskmanager_1         |[0m INFO  [] -           Network:               128.000mb (134217730 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -     JVM Metaspace:               256.000mb (268435456 bytes)
[36;1mtaskmanager_1         |[0m INFO  [] -     JVM Overhead:                192.000mb (201326592 bytes)
[36;1mtaskmanager_1         |[0m 
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,468 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,468 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Starting TaskManager (Version: 1.14.6, Scala: 2.12, Rev:a921a4d, Date:2022-09-09T10:18:38+02:00)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,469 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  OS current user: flink
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,470 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Current Hadoop/Kerberos user: <no hadoop dependency found>
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,470 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JVM: OpenJDK 64-Bit Server VM - Temurin - 1.8/25.345-b01
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,471 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Maximum heap size: 512 MiBytes
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,471 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JAVA_HOME: /opt/java/openjdk
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,473 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  No Hadoop Dependency available
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,474 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  JVM Options:
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,476 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:+UseG1GC
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,476 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Xmx536870902
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,477 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Xms536870902
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,482 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:MaxDirectMemorySize=268435458
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,483 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -XX:MaxMetaspaceSize=268435456
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,483 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog.file=/opt/flink/log/flink--taskexecutor-1-a189f5323dec.log
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,483 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog4j.configuration=file:/opt/flink/conf/log4j-console.properties
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,484 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,484 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -Dlogback.configurationFile=file:/opt/flink/conf/logback-console.xml
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,485 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Program Arguments:
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,487 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     --configDir
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,490 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     /opt/flink/conf
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,490 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,492 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.network.min=134217730b
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,495 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,496 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.cpu.cores=1.0
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,496 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,497 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.task.off-heap.size=0b
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,497 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,498 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-metaspace.size=268435456b
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,499 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,499 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     external-resources=none
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,500 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,503 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-overhead.min=201326592b
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,504 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,505 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.framework.off-heap.size=134217728b
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,506 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,507 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.network.max=134217730b
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,507 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,508 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.framework.heap.size=134217728b
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,511 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,512 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.managed.size=536870920b
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,513 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,514 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.task.heap.size=402653174b
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,522 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,523 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.numberOfTaskSlots=1
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,524 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     -D
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,524 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -     taskmanager.memory.jvm-overhead.max=201326592b
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,525 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Classpath: /opt/flink/lib/flink-csv-1.14.6.jar:/opt/flink/lib/flink-json-1.14.6.jar:/opt/flink/lib/flink-shaded-zookeeper-3.4.14.jar:/opt/flink/lib/flink-table_2.12-1.14.6.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/flink-dist_2.12-1.14.6.jar:::
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,525 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,527 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Registered UNIX signal handlers for [TERM, HUP, INT]
[33mjobmanager_1          |[0m 2023-03-28 11:12:37,532 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address jobmanager:6123, bind address 0.0.0.0:6123.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,540 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Maximum number of open file descriptors is 1048576.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,582 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1600Mb
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,582 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, jobmanager
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,583 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: blob.server.port, 6124
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,584 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: query.server.port, 6125
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,584 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728Mb
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,585 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,586 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend, filesystem
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,611 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.checkpoints.dir, file:///tmp/flink-checkpoints-directory
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,613 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.savepoints.dir, file:///tmp/flink-savepoints-directory
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,616 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: heartbeat.interval, 1000
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,618 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: heartbeat.timeout, 5000
[32mkafka_1               |[0m [2023-03-28 11:12:37,787] INFO Awaiting socket connections on s0.0.0.0:9092. (kafka.network.Acceptor)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,811 INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
[32mkafka_1               |[0m [2023-03-28 11:12:37,911] INFO [SocketServer brokerId=1001] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(INSIDE),PLAINTEXT) (kafka.network.SocketServer)
[32mkafka_1               |[0m [2023-03-28 11:12:37,912] INFO Awaiting socket connections on s0.0.0.0:9094. (kafka.network.Acceptor)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,986 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory}.
[32mkafka_1               |[0m [2023-03-28 11:12:37,988] INFO [SocketServer brokerId=1001] Created data-plane acceptor and processors for endpoint : EndPoint(null,9094,ListenerName(OUTSIDE),PLAINTEXT) (kafka.network.SocketServer)
[32mkafka_1               |[0m [2023-03-28 11:12:37,991] INFO [SocketServer brokerId=1001] Started 2 acceptor threads for data-plane (kafka.network.SocketServer)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:37,997 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - StateChangelogStorageLoader initialized with shortcut names {memory}.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:38,027 INFO  org.apache.flink.runtime.security.modules.HadoopModuleFactory [] - Cannot create Hadoop Security Module because Hadoop cannot be found in the Classpath.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:38,052 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-53095033868558540.conf.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:38,078 INFO  org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory [] - Cannot install HadoopSecurityContext because Hadoop cannot be found in the Classpath.
[32mkafka_1               |[0m [2023-03-28 11:12:38,100] INFO [ExpirationReaper-1001-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1               |[0m [2023-03-28 11:12:38,104] INFO [ExpirationReaper-1001-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1               |[0m [2023-03-28 11:12:38,111] INFO [ExpirationReaper-1001-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1               |[0m [2023-03-28 11:12:38,111] INFO [ExpirationReaper-1001-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1               |[0m [2023-03-28 11:12:38,200] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[32mkafka_1               |[0m [2023-03-28 11:12:38,304] INFO Creating /brokers/ids/1001 (is it secure? false) (kafka.zk.KafkaZkClient)
[32mkafka_1               |[0m [2023-03-28 11:12:38,378] INFO Stat of the created znode at /brokers/ids/1001 is: 25,25,1680001958339,1680001958339,1,0,0,110100607652397056,240,0,25
[32mkafka_1               |[0m  (kafka.zk.KafkaZkClient)
[32mkafka_1               |[0m [2023-03-28 11:12:38,380] INFO Registered broker 1001 at path /brokers/ids/1001 with addresses: ArrayBuffer(EndPoint(d8ba92357be3,9092,ListenerName(INSIDE),PLAINTEXT), EndPoint(d8ba92357be3,9094,ListenerName(OUTSIDE),PLAINTEXT)), czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)
[32mkafka_1               |[0m [2023-03-28 11:12:38,384] WARN No meta.properties file under dir /kafka/kafka-logs-d8ba92357be3/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:38,803 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'
[32mkafka_1               |[0m [2023-03-28 11:12:38,820] INFO [ExpirationReaper-1001-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:38,831 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - Trying to select the network interface and address to use by connecting to the leading JobManager.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:38,832 INFO  org.apache.flink.runtime.util.LeaderRetrievalUtils           [] - TaskManager will try to connect for PT10S before falling back to heuristics
[32mkafka_1               |[0m [2023-03-28 11:12:38,846] INFO [ExpirationReaper-1001-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1               |[0m [2023-03-28 11:12:38,860] INFO [ExpirationReaper-1001-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1               |[0m [2023-03-28 11:12:38,878] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[32mkafka_1               |[0m [2023-03-28 11:12:38,945] INFO [GroupCoordinator 1001]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:12:38,970] INFO [GroupCoordinator 1001]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:12:39,127] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 156 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:12:39,154] INFO [ProducerId Manager 1001]: Acquired new producerId block (brokerId:1001,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[32mkafka_1               |[0m [2023-03-28 11:12:39,293] INFO [TransactionCoordinator id=1001] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:12:39,306] INFO [TransactionCoordinator id=1001] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,318 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/172.31.0.5:6123
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,319 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [a189f5323dec/172.31.0.6] with timeout [200] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,320 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/172.31.0.6] with timeout [50] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,321 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/172.31.0.6] with timeout [50] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,322 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,323 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/172.31.0.6] with timeout [1000] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,323 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
[32mkafka_1               |[0m [2023-03-28 11:12:39,333] INFO [Transaction Marker Channel Manager 1001]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[33mjobmanager_1          |[0m 2023-03-28 11:12:39,351 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,424 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/172.31.0.5:6123
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,425 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [a189f5323dec/172.31.0.6] with timeout [200] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,426 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/172.31.0.6] with timeout [50] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,427 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/172.31.0.6] with timeout [50] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,428 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,430 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/172.31.0.6] with timeout [1000] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,431 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
[33mjobmanager_1          |[0m 2023-03-28 11:12:39,446 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
[33mjobmanager_1          |[0m 2023-03-28 11:12:39,447 INFO  akka.remote.Remoting                                         [] - Starting remoting
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:39,472 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@592] - Got user-level KeeperException when processing sessionid:0x18727eb58ad0000 type:multi cxid:0x33 zxid:0x1d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
[32mkafka_1               |[0m [2023-03-28 11:12:39,506] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[32mkafka_1               |[0m [2023-03-28 11:12:39,526] INFO [SocketServer brokerId=1001] Started data-plane processors for 2 acceptors (kafka.network.SocketServer)
[32mkafka_1               |[0m [2023-03-28 11:12:39,533] INFO Kafka version: 2.2.1 (org.apache.kafka.common.utils.AppInfoParser)
[32mkafka_1               |[0m [2023-03-28 11:12:39,533] INFO Kafka commitId: 55783d3133a5a49a (org.apache.kafka.common.utils.AppInfoParser)
[32mkafka_1               |[0m [2023-03-28 11:12:39,550] INFO [KafkaServer id=1001] started (kafka.server.KafkaServer)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,632 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/172.31.0.5:6123
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,633 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [a189f5323dec/172.31.0.6] with timeout [200] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,634 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/172.31.0.6] with timeout [50] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,635 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/172.31.0.6] with timeout [50] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,635 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/127.0.0.1] with timeout [50] due to: Invalid argument (connect failed)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,636 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/172.31.0.6] with timeout [1000] due to: Connection refused (Connection refused)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:39,637 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Failed to connect to [jobmanager/172.31.0.5:6123] from local address [/127.0.0.1] with timeout [1000] due to: Invalid argument (connect failed)
[32mkafka_1               |[0m [2023-03-28 11:12:39,736] INFO Creating topic server-logs with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[33mjobmanager_1          |[0m 2023-03-28 11:12:39,738 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@jobmanager:6123]
[33;1mzookeeper_1           |[0m 2023-03-28 11:12:39,740 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x18727eb58ad0000 type:setData cxid:0x41 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/server-logs Error:KeeperErrorCode = NoNode for /config/topics/server-logs
[32mkafka_1               |[0m [2023-03-28 11:12:39,826] INFO [KafkaApi-1001] Auto creation of topic server-logs with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[33mjobmanager_1          |[0m 2023-03-28 11:12:39,907 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@jobmanager:6123
[32mkafka_1               |[0m [2023-03-28 11:12:39,929] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions Set(server-logs-0) (kafka.server.ReplicaFetcherManager)
[33mjobmanager_1          |[0m 2023-03-28 11:12:39,956 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'
[33mjobmanager_1          |[0m 2023-03-28 11:12:39,968 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory /tmp/blobStore-4ec7310a-1610-420d-8f63-7eff4907648b
[33mjobmanager_1          |[0m 2023-03-28 11:12:39,982 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:6124 - max concurrent requests: 50 - max backlog: 1000
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,002 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,007 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address jobmanager:0, bind address 0.0.0.0:0.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:40,037 INFO  org.apache.flink.runtime.net.ConnectionUtils                 [] - Trying to connect to address jobmanager/172.31.0.5:6123
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:40,038 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - TaskManager will use hostname/address 'a189f5323dec' (172.31.0.6) for communication.
[32mkafka_1               |[0m [2023-03-28 11:12:40,061] INFO [Log partition=server-logs-0, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,075 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
[32mkafka_1               |[0m [2023-03-28 11:12:40,083] INFO [Log partition=server-logs-0, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 107 ms (kafka.log.Log)
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,086 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
[32mkafka_1               |[0m [2023-03-28 11:12:40,091] INFO Created log for partition server-logs-0 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:12:40,094] INFO [Partition server-logs-0 broker=1001] No checkpointed highwatermark is found for partition server-logs-0 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:12:40,097] INFO Replica loaded for partition server-logs-0 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:12:40,109] INFO [Partition server-logs-0 broker=1001] server-logs-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:40,113 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 172.31.0.6:0, bind address 0.0.0.0:0.
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,121 INFO  akka.remote.Remoting                                         [] - Starting remoting
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,231 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@jobmanager:42479]
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,250 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@jobmanager:42479
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,274 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,316 INFO  org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStore [] - Initializing FileExecutionGraphInfoStore: Storage directory /tmp/executionGraphStore-3b86a898-effd-4928-87e5-3c74374f7066, expiration time 3600000, maximum cache size 52428800 bytes.
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,472 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,474 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Upload directory /tmp/flink-web-33ec5c34-432d-4b9d-9c03-c8927ef6115e/flink-web-upload does not exist. 
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,475 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Created directory /tmp/flink-web-33ec5c34-432d-4b9d-9c03-c8927ef6115e/flink-web-upload for file uploads.
[33mjobmanager_1          |[0m 2023-03-28 11:12:40,479 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Starting rest endpoint.
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,148 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component log file: /opt/flink/log/flink--standalonesession-1-75e532f08cbd.log
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,148 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component stdout file: /opt/flink/log/flink--standalonesession-1-75e532f08cbd.out
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:41,633 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,702 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Rest endpoint listening at jobmanager:8081
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,705 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - http://jobmanager:8081 was granted leadership with leaderSessionID=00000000-0000-0000-0000-000000000000
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:41,724 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:41,741 INFO  akka.remote.Remoting                                         [] - Starting remoting
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,748 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Web frontend listening at http://jobmanager:8081.
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,827 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id 00000000-0000-0000-0000-000000000000. Creating new DispatcherLeaderProcess.
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,835 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,901 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,951 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,951 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
[33mjobmanager_1          |[0m 2023-03-28 11:12:41,970 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 00000000-0000-0000-0000-000000000000.
[33mjobmanager_1          |[0m 2023-03-28 11:12:42,012 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_0 .
[33mjobmanager_1          |[0m 2023-03-28 11:12:42,089 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
[33mjobmanager_1          |[0m 2023-03-28 11:12:42,168 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:42,204 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@172.31.0.6:37697]
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:42,617 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@172.31.0.6:37697
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:42,716 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:42,731 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address 172.31.0.6:0, bind address 0.0.0.0:0.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:42,903 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:42,941 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:42,942 INFO  akka.remote.Remoting                                         [] - Starting remoting
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,051 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@172.31.0.6:39841]
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,132 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@172.31.0.6:39841
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,221 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService_172.31.0.6:37697-5eacc7 .
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,279 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory /tmp/blobStore-502d7043-ca88-4ee5-b711-ffca30957867
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,358 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory /tmp/blobStore-c7d284fc-187a-40ad-a42d-680a4e5e70c8
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,362 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,371 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: 172.31.0.6:37697-5eacc7
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,445 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory '/tmp': total 250 GB, usable 221 GB (88.40% usable)
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,451 INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager         [] - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
[36;1mtaskmanager_1         |[0m 	/tmp/flink-io-aa402dc5-8f3f-4e9c-b90c-911b72453616
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,490 INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: /0.0.0.0, server port: 0, ssl enabled: false, memory segment size (bytes): 32768, transport type: AUTO, number of server threads: 1 (manual), number of client threads: 1 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:43,818 INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory [] - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
[36;1mtaskmanager_1         |[0m 	/tmp/flink-netty-shuffle-e9c962d3-0c6a-41f2-97f8-b06602e2d326
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,004 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 128 MB for network buffer pool (number of memory segments: 4096, bytes per segment: 32768).
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,032 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,220 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,223 INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 191 ms).
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,231 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,305 INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 77 ms). Listening on SocketAddress /0.0.0.0:35089.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,310 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,410 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,475 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,478 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory /tmp/flink-dist-cache-5bf286e6-ec48-4e59-b5ad-2d3a2c45a90a
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:44,484 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000).
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:45,217 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration
[33mjobmanager_1          |[0m 2023-03-28 11:12:45,606 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 172.31.0.6:37697-5eacc7 (akka.tcp://flink@172.31.0.6:37697/user/rpc/taskmanager_0) at ResourceManager
[33mjobmanager_1          |[0m 2023-03-28 11:12:45,678 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 172.31.0.6:37697-5eacc7 (akka.tcp://flink@172.31.0.6:37697/user/rpc/taskmanager_0) at ResourceManager
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:45,698 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_* under registration id 2719c19eb3d1b880a861aa6fa0e4d7d8.
[32mkafka_1               |[0m creating topics: server-logs
[32mkafka_1               |[0m creating topics:  alerts
[33mjobmanager_1          |[0m 2023-03-28 11:12:47,228 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'Fraud Detection' (bfe74f5a3dd0e511ceff6853162ed9dd).
[33mjobmanager_1          |[0m 2023-03-28 11:12:47,229 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'Fraud Detection' (bfe74f5a3dd0e511ceff6853162ed9dd).
[33mjobmanager_1          |[0m 2023-03-28 11:12:47,396 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .
[33mjobmanager_1          |[0m 2023-03-28 11:12:47,471 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'Fraud Detection' (bfe74f5a3dd0e511ceff6853162ed9dd).
[33mjobmanager_1          |[0m 2023-03-28 11:12:47,667 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for Fraud Detection (bfe74f5a3dd0e511ceff6853162ed9dd).
[33mjobmanager_1          |[0m 2023-03-28 11:12:47,928 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job Fraud Detection (bfe74f5a3dd0e511ceff6853162ed9dd).
[33mjobmanager_1          |[0m 2023-03-28 11:12:47,928 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,210 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 pipelined regions in 21 ms
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,347 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using job/cluster config to configure application-defined state backend: org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5d34a4db
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,354 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using application-defined state backend: org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6a3daa3d
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,355 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,381 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using job/cluster config to configure application-defined checkpoint storage: org.apache.flink.runtime.state.storage.FileSystemCheckpointStorage@26bdeb57
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,451 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,478 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@1ea9fd6 for Fraud Detection (bfe74f5a3dd0e511ceff6853162ed9dd).
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,557 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'Fraud Detection' (bfe74f5a3dd0e511ceff6853162ed9dd) under job master id 00000000000000000000000000000000.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,599 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,600 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Fraud Detection (bfe74f5a3dd0e511ceff6853162ed9dd) switched from state CREATED to RUNNING.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,634 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: incoming-events -> Sink: event-log (1/2) (67082fafc4ef711d379774e220eb3a77) switched from CREATED to SCHEDULED.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,646 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: incoming-events -> Sink: event-log (2/2) (4c445d5e62cfe4f736eed1555839d7c5) switched from CREATED to SCHEDULED.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,647 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - fraud-detector -> Sink: send-alerts (1/2) (5204fc218efe3c04aab7880bcf98e43c) switched from CREATED to SCHEDULED.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,649 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - fraud-detector -> Sink: send-alerts (2/2) (b39819125cf6b9d802a155ce8a4f54a9) switched from CREATED to SCHEDULED.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,751 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,768 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,773 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job bfe74f5a3dd0e511ceff6853162ed9dd.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,795 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job bfe74f5a3dd0e511ceff6853162ed9dd.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,846 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.
[33mjobmanager_1          |[0m 2023-03-28 11:12:48,857 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job bfe74f5a3dd0e511ceff6853162ed9dd: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:48,890 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request cf9088bc502bdce73989248dde6339d2 for job bfe74f5a3dd0e511ceff6853162ed9dd from resource manager with leader id 00000000000000000000000000000000.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:48,910 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for cf9088bc502bdce73989248dde6339d2.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:48,914 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job bfe74f5a3dd0e511ceff6853162ed9dd for job leader monitoring.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:48,918 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 with leader id 00000000-0000-0000-0000-000000000000.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:48,958 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:49,033 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job bfe74f5a3dd0e511ceff6853162ed9dd.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:49,038 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job bfe74f5a3dd0e511ceff6853162ed9dd.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:49,053 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job bfe74f5a3dd0e511ceff6853162ed9dd.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:12:49,142 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot cf9088bc502bdce73989248dde6339d2.
[36mfrauddetection_1      |[0m Job has been submitted with JobID bfe74f5a3dd0e511ceff6853162ed9dd
[36mde_project_stream_frauddetection_1 exited with code 0
[0m[32mkafka_1               |[0m Missing required argument "[replication-factor]"
[32mkafka_1               |[0m Option                                   Description                            
[32mkafka_1               |[0m ------                                   -----------                            
[32mkafka_1               |[0m --alter                                  Alter the number of partitions,        
[32mkafka_1               |[0m                                            replica assignment, and/or           
[32mkafka_1               |[0m                                            configuration for the topic.         
[32mkafka_1               |[0m --bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  
[32mkafka_1               |[0m   connect to>                              to. In case of providing this, a     
[32mkafka_1               |[0m                                            direct Zookeeper connection won't be 
[32mkafka_1               |[0m                                            required.                            
[32mkafka_1               |[0m --command-config <String: command        Property file containing configs to be 
[32mkafka_1               |[0m   config property file>                    passed to Admin Client. This is used 
[32mkafka_1               |[0m                                            only with --bootstrap-server option  
[32mkafka_1               |[0m                                            for describing and altering broker   
[32mkafka_1               |[0m                                            configs.                             
[32mkafka_1               |[0m --config <String: name=value>            A topic configuration override for the 
[32mkafka_1               |[0m                                            topic being created or altered.The   
[32mkafka_1               |[0m                                            following is a list of valid         
[32mkafka_1               |[0m                                            configurations:                      
[32mkafka_1               |[0m                                          	cleanup.policy                        
[32mkafka_1               |[0m                                          	compression.type                      
[32mkafka_1               |[0m                                          	delete.retention.ms                   
[32mkafka_1               |[0m                                          	file.delete.delay.ms                  
[32mkafka_1               |[0m                                          	flush.messages                        
[32mkafka_1               |[0m                                          	flush.ms                              
[32mkafka_1               |[0m                                          	follower.replication.throttled.       
[32mkafka_1               |[0m                                            replicas                             
[32mkafka_1               |[0m                                          	index.interval.bytes                  
[32mkafka_1               |[0m                                          	leader.replication.throttled.replicas 
[32mkafka_1               |[0m                                          	max.message.bytes                     
[32mkafka_1               |[0m                                          	message.downconversion.enable         
[32mkafka_1               |[0m                                          	message.format.version                
[32mkafka_1               |[0m                                          	message.timestamp.difference.max.ms   
[32mkafka_1               |[0m                                          	message.timestamp.type                
[32mkafka_1               |[0m                                          	min.cleanable.dirty.ratio             
[32mkafka_1               |[0m                                          	min.compaction.lag.ms                 
[32mkafka_1               |[0m                                          	min.insync.replicas                   
[32mkafka_1               |[0m                                          	preallocate                           
[32mkafka_1               |[0m                                          	retention.bytes                       
[32mkafka_1               |[0m                                          	retention.ms                          
[32mkafka_1               |[0m                                          	segment.bytes                         
[32mkafka_1               |[0m                                          	segment.index.bytes                   
[32mkafka_1               |[0m                                          	segment.jitter.ms                     
[32mkafka_1               |[0m                                          	segment.ms                            
[32mkafka_1               |[0m                                          	unclean.leader.election.enable        
[32mkafka_1               |[0m                                          See the Kafka documentation for full   
[32mkafka_1               |[0m                                            details on the topic configs.It is   
[32mkafka_1               |[0m                                            supported only in combination with --
[32mkafka_1               |[0m                                            create if --bootstrap-server option  
[32mkafka_1               |[0m                                            is used.                             
[32mkafka_1               |[0m --create                                 Create a new topic.                    
[32mkafka_1               |[0m --delete                                 Delete a topic                         
[32mkafka_1               |[0m --delete-config <String: name>           A topic configuration override to be   
[32mkafka_1               |[0m                                            removed for an existing topic (see   
[32mkafka_1               |[0m                                            the list of configurations under the 
[32mkafka_1               |[0m                                            --config option). Not supported with 
[32mkafka_1               |[0m                                            the --bootstrap-server option.       
[32mkafka_1               |[0m --describe                               List details for the given topics.     
[32mkafka_1               |[0m --disable-rack-aware                     Disable rack aware replica assignment  
[32mkafka_1               |[0m --exclude-internal                       exclude internal topics when running   
[32mkafka_1               |[0m                                            list or describe command. The        
[32mkafka_1               |[0m                                            internal topics will be listed by    
[32mkafka_1               |[0m                                            default                              
[32mkafka_1               |[0m --force                                  Suppress console prompts               
[32mkafka_1               |[0m --help                                   Print usage information.               
[32mkafka_1               |[0m --if-exists                              if set when altering or deleting or    
[32mkafka_1               |[0m                                            describing topics, the action will   
[32mkafka_1               |[0m                                            only execute if the topic exists.    
[32mkafka_1               |[0m                                            Not supported with the --bootstrap-  
[32mkafka_1               |[0m                                            server option.                       
[32mkafka_1               |[0m --if-not-exists                          if set when creating topics, the       
[32mkafka_1               |[0m                                            action will only execute if the      
[32mkafka_1               |[0m                                            topic does not already exist. Not    
[32mkafka_1               |[0m                                            supported with the --bootstrap-      
[32mkafka_1               |[0m                                            server option.                       
[32mkafka_1               |[0m --list                                   List all available topics.             
[32mkafka_1               |[0m --partitions <Integer: # of partitions>  The number of partitions for the topic 
[32mkafka_1               |[0m                                            being created or altered (WARNING:   
[32mkafka_1               |[0m                                            If partitions are increased for a    
[32mkafka_1               |[0m                                            topic that has a key, the partition  
[32mkafka_1               |[0m                                            logic or ordering of the messages    
[32mkafka_1               |[0m                                            will be affected                     
[32mkafka_1               |[0m --replica-assignment <String:            A list of manual partition-to-broker   
[32mkafka_1               |[0m   broker_id_for_part1_replica1 :           assignments for the topic being      
[32mkafka_1               |[0m   broker_id_for_part1_replica2 ,           created or altered.                  
[32mkafka_1               |[0m   broker_id_for_part2_replica1 :                                                
[32mkafka_1               |[0m   broker_id_for_part2_replica2 , ...>                                           
[32mkafka_1               |[0m --replication-factor <Integer:           The replication factor for each        
[32mkafka_1               |[0m   replication factor>                      partition in the topic being created.
[32mkafka_1               |[0m --topic <String: topic>                  The topic to create, alter, describe   
[32mkafka_1               |[0m                                            or delete. It also accepts a regular 
[32mkafka_1               |[0m                                            expression, except for --create      
[32mkafka_1               |[0m                                            option. Put topic name in double     
[32mkafka_1               |[0m                                            quotes and use the '\' prefix to     
[32mkafka_1               |[0m                                            escape regular expression symbols; e.
[32mkafka_1               |[0m                                            g. "test\.topic".                    
[32mkafka_1               |[0m --topics-with-overrides                  if set when describing topics, only    
[32mkafka_1               |[0m                                            show topics that have overridden     
[32mkafka_1               |[0m                                            configs                              
[32mkafka_1               |[0m --unavailable-partitions                 if set when describing topics, only    
[32mkafka_1               |[0m                                            show partitions whose leader is not  
[32mkafka_1               |[0m                                            available                            
[32mkafka_1               |[0m --under-replicated-partitions            if set when describing topics, only    
[32mkafka_1               |[0m                                            show under replicated partitions     
[32mkafka_1               |[0m --zookeeper <String: hosts>              DEPRECATED, The connection string for  
[32mkafka_1               |[0m                                            the zookeeper connection in the form 
[32mkafka_1               |[0m                                            host:port. Multiple hosts can be     
[32mkafka_1               |[0m                                            given to allow fail-over.            
[32mkafka_1               |[0m Missing required argument "[replication-factor]"
[32mkafka_1               |[0m Option                                   Description                            
[32mkafka_1               |[0m ------                                   -----------                            
[32mkafka_1               |[0m --alter                                  Alter the number of partitions,        
[32mkafka_1               |[0m                                            replica assignment, and/or           
[32mkafka_1               |[0m                                            configuration for the topic.         
[32mkafka_1               |[0m --bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  
[32mkafka_1               |[0m   connect to>                              to. In case of providing this, a     
[32mkafka_1               |[0m                                            direct Zookeeper connection won't be 
[32mkafka_1               |[0m                                            required.                            
[32mkafka_1               |[0m --command-config <String: command        Property file containing configs to be 
[32mkafka_1               |[0m   config property file>                    passed to Admin Client. This is used 
[32mkafka_1               |[0m                                            only with --bootstrap-server option  
[32mkafka_1               |[0m                                            for describing and altering broker   
[32mkafka_1               |[0m                                            configs.                             
[32mkafka_1               |[0m --config <String: name=value>            A topic configuration override for the 
[32mkafka_1               |[0m                                            topic being created or altered.The   
[32mkafka_1               |[0m                                            following is a list of valid         
[32mkafka_1               |[0m                                            configurations:                      
[32mkafka_1               |[0m                                          	cleanup.policy                        
[32mkafka_1               |[0m                                          	compression.type                      
[32mkafka_1               |[0m                                          	delete.retention.ms                   
[32mkafka_1               |[0m                                          	file.delete.delay.ms                  
[32mkafka_1               |[0m                                          	flush.messages                        
[32mkafka_1               |[0m                                          	flush.ms                              
[32mkafka_1               |[0m                                          	follower.replication.throttled.       
[32mkafka_1               |[0m                                            replicas                             
[32mkafka_1               |[0m                                          	index.interval.bytes                  
[32mkafka_1               |[0m                                          	leader.replication.throttled.replicas 
[32mkafka_1               |[0m                                          	max.message.bytes                     
[32mkafka_1               |[0m                                          	message.downconversion.enable         
[32mkafka_1               |[0m                                          	message.format.version                
[32mkafka_1               |[0m                                          	message.timestamp.difference.max.ms   
[32mkafka_1               |[0m                                          	message.timestamp.type                
[32mkafka_1               |[0m                                          	min.cleanable.dirty.ratio             
[32mkafka_1               |[0m                                          	min.compaction.lag.ms                 
[32mkafka_1               |[0m                                          	min.insync.replicas                   
[32mkafka_1               |[0m                                          	preallocate                           
[32mkafka_1               |[0m                                          	retention.bytes                       
[32mkafka_1               |[0m                                          	retention.ms                          
[32mkafka_1               |[0m                                          	segment.bytes                         
[32mkafka_1               |[0m                                          	segment.index.bytes                   
[32mkafka_1               |[0m                                          	segment.jitter.ms                     
[32mkafka_1               |[0m                                          	segment.ms                            
[32mkafka_1               |[0m                                          	unclean.leader.election.enable        
[32mkafka_1               |[0m                                          See the Kafka documentation for full   
[32mkafka_1               |[0m                                            details on the topic configs.It is   
[32mkafka_1               |[0m                                            supported only in combination with --
[32mkafka_1               |[0m                                            create if --bootstrap-server option  
[32mkafka_1               |[0m                                            is used.                             
[32mkafka_1               |[0m --create                                 Create a new topic.                    
[32mkafka_1               |[0m --delete                                 Delete a topic                         
[32mkafka_1               |[0m --delete-config <String: name>           A topic configuration override to be   
[32mkafka_1               |[0m                                            removed for an existing topic (see   
[32mkafka_1               |[0m                                            the list of configurations under the 
[32mkafka_1               |[0m                                            --config option). Not supported with 
[32mkafka_1               |[0m                                            the --bootstrap-server option.       
[32mkafka_1               |[0m --describe                               List details for the given topics.     
[32mkafka_1               |[0m --disable-rack-aware                     Disable rack aware replica assignment  
[32mkafka_1               |[0m --exclude-internal                       exclude internal topics when running   
[32mkafka_1               |[0m                                            list or describe command. The        
[32mkafka_1               |[0m                                            internal topics will be listed by    
[32mkafka_1               |[0m                                            default                              
[32mkafka_1               |[0m --force                                  Suppress console prompts               
[32mkafka_1               |[0m --help                                   Print usage information.               
[32mkafka_1               |[0m --if-exists                              if set when altering or deleting or    
[32mkafka_1               |[0m                                            describing topics, the action will   
[32mkafka_1               |[0m                                            only execute if the topic exists.    
[32mkafka_1               |[0m                                            Not supported with the --bootstrap-  
[32mkafka_1               |[0m                                            server option.                       
[32mkafka_1               |[0m --if-not-exists                          if set when creating topics, the       
[32mkafka_1               |[0m                                            action will only execute if the      
[32mkafka_1               |[0m                                            topic does not already exist. Not    
[32mkafka_1               |[0m                                            supported with the --bootstrap-      
[32mkafka_1               |[0m                                            server option.                       
[32mkafka_1               |[0m --list                                   List all available topics.             
[32mkafka_1               |[0m --partitions <Integer: # of partitions>  The number of partitions for the topic 
[32mkafka_1               |[0m                                            being created or altered (WARNING:   
[32mkafka_1               |[0m                                            If partitions are increased for a    
[32mkafka_1               |[0m                                            topic that has a key, the partition  
[32mkafka_1               |[0m                                            logic or ordering of the messages    
[32mkafka_1               |[0m                                            will be affected                     
[32mkafka_1               |[0m --replica-assignment <String:            A list of manual partition-to-broker   
[32mkafka_1               |[0m   broker_id_for_part1_replica1 :           assignments for the topic being      
[32mkafka_1               |[0m   broker_id_for_part1_replica2 ,           created or altered.                  
[32mkafka_1               |[0m   broker_id_for_part2_replica1 :                                                
[32mkafka_1               |[0m   broker_id_for_part2_replica2 , ...>                                           
[32mkafka_1               |[0m --replication-factor <Integer:           The replication factor for each        
[32mkafka_1               |[0m   replication factor>                      partition in the topic being created.
[32mkafka_1               |[0m --topic <String: topic>                  The topic to create, alter, describe   
[32mkafka_1               |[0m                                            or delete. It also accepts a regular 
[32mkafka_1               |[0m                                            expression, except for --create      
[32mkafka_1               |[0m                                            option. Put topic name in double     
[32mkafka_1               |[0m                                            quotes and use the '\' prefix to     
[32mkafka_1               |[0m                                            escape regular expression symbols; e.
[32mkafka_1               |[0m                                            g. "test\.topic".                    
[32mkafka_1               |[0m --topics-with-overrides                  if set when describing topics, only    
[32mkafka_1               |[0m                                            show topics that have overridden     
[32mkafka_1               |[0m                                            configs                              
[32mkafka_1               |[0m --unavailable-partitions                 if set when describing topics, only    
[32mkafka_1               |[0m                                            show partitions whose leader is not  
[32mkafka_1               |[0m                                            available                            
[32mkafka_1               |[0m --under-replicated-partitions            if set when describing topics, only    
[32mkafka_1               |[0m                                            show under replicated partitions     
[32mkafka_1               |[0m --zookeeper <String: hosts>              DEPRECATED, The connection string for  
[32mkafka_1               |[0m                                            the zookeeper connection in the form 
[32mkafka_1               |[0m                                            host:port. Multiple hosts can be     
[32mkafka_1               |[0m                                            given to allow fail-over.            
[34mde_project_stream_serverloggenerator_1 exited with code 0
[0m[32mkafka_1               |[0m [2023-03-28 11:13:51,471] INFO Creating topic alerts with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[33;1mzookeeper_1           |[0m 2023-03-28 11:13:51,474 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x18727eb58ad0000 type:setData cxid:0x56 zxid:0x24 txntype:-1 reqpath:n/a Error Path:/config/topics/alerts Error:KeeperErrorCode = NoNode for /config/topics/alerts
[32mkafka_1               |[0m [2023-03-28 11:13:51,489] INFO [KafkaApi-1001] Auto creation of topic alerts with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[32mkafka_1               |[0m [2023-03-28 11:13:51,514] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(23 -> ArrayBuffer(1001), 32 -> ArrayBuffer(1001), 41 -> ArrayBuffer(1001), 17 -> ArrayBuffer(1001), 8 -> ArrayBuffer(1001), 35 -> ArrayBuffer(1001), 44 -> ArrayBuffer(1001), 26 -> ArrayBuffer(1001), 11 -> ArrayBuffer(1001), 29 -> ArrayBuffer(1001), 38 -> ArrayBuffer(1001), 47 -> ArrayBuffer(1001), 20 -> ArrayBuffer(1001), 2 -> ArrayBuffer(1001), 5 -> ArrayBuffer(1001), 14 -> ArrayBuffer(1001), 46 -> ArrayBuffer(1001), 49 -> ArrayBuffer(1001), 40 -> ArrayBuffer(1001), 13 -> ArrayBuffer(1001), 4 -> ArrayBuffer(1001), 22 -> ArrayBuffer(1001), 31 -> ArrayBuffer(1001), 16 -> ArrayBuffer(1001), 7 -> ArrayBuffer(1001), 43 -> ArrayBuffer(1001), 25 -> ArrayBuffer(1001), 34 -> ArrayBuffer(1001), 10 -> ArrayBuffer(1001), 37 -> ArrayBuffer(1001), 1 -> ArrayBuffer(1001), 19 -> ArrayBuffer(1001), 28 -> ArrayBuffer(1001), 45 -> ArrayBuffer(1001), 27 -> ArrayBuffer(1001), 36 -> ArrayBuffer(1001), 18 -> ArrayBuffer(1001), 9 -> ArrayBuffer(1001), 21 -> ArrayBuffer(1001), 48 -> ArrayBuffer(1001), 3 -> ArrayBuffer(1001), 12 -> ArrayBuffer(1001), 30 -> ArrayBuffer(1001), 39 -> ArrayBuffer(1001), 15 -> ArrayBuffer(1001), 42 -> ArrayBuffer(1001), 24 -> ArrayBuffer(1001), 6 -> ArrayBuffer(1001), 33 -> ArrayBuffer(1001), 0 -> ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[33;1mzookeeper_1           |[0m 2023-03-28 11:13:51,516 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x18727eb58ad0000 type:setData cxid:0x62 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
[32mkafka_1               |[0m [2023-03-28 11:13:51,522] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions Set(alerts-0) (kafka.server.ReplicaFetcherManager)
[32mkafka_1               |[0m [2023-03-28 11:13:51,535] INFO [Log partition=alerts-0, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,536] INFO [Log partition=alerts-0, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,538] INFO Created log for partition alerts-0 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:51,539] INFO [Partition alerts-0 broker=1001] No checkpointed highwatermark is found for partition alerts-0 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,542] INFO Replica loaded for partition alerts-0 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:51,543] INFO [Partition alerts-0 broker=1001] alerts-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,549] INFO [KafkaApi-1001] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[32mkafka_1               |[0m [2023-03-28 11:13:51,910] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[32mkafka_1               |[0m [2023-03-28 11:13:51,915] INFO [Log partition=__consumer_offsets-0, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,917] INFO [Log partition=__consumer_offsets-0, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,918] INFO Created log for partition __consumer_offsets-0 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:51,919] INFO [Partition __consumer_offsets-0 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,919] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:51,920] INFO [Partition __consumer_offsets-0 broker=1001] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,941] INFO [Log partition=__consumer_offsets-29, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,942] INFO [Log partition=__consumer_offsets-29, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,944] INFO Created log for partition __consumer_offsets-29 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:51,945] INFO [Partition __consumer_offsets-29 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,945] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:51,946] INFO [Partition __consumer_offsets-29 broker=1001] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,959] INFO [Log partition=__consumer_offsets-48, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,961] INFO [Log partition=__consumer_offsets-48, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,962] INFO Created log for partition __consumer_offsets-48 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:51,962] INFO [Partition __consumer_offsets-48 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,963] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:51,964] INFO [Partition __consumer_offsets-48 broker=1001] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,973] INFO [Log partition=__consumer_offsets-10, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,975] INFO [Log partition=__consumer_offsets-10, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,976] INFO Created log for partition __consumer_offsets-10 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:51,976] INFO [Partition __consumer_offsets-10 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,977] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:51,977] INFO [Partition __consumer_offsets-10 broker=1001] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,987] INFO [Log partition=__consumer_offsets-45, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,988] INFO [Log partition=__consumer_offsets-45, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:51,991] INFO Created log for partition __consumer_offsets-45 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:51,992] INFO [Partition __consumer_offsets-45 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:51,992] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:51,993] INFO [Partition __consumer_offsets-45 broker=1001] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,006] INFO [Log partition=__consumer_offsets-26, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,008] INFO [Log partition=__consumer_offsets-26, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,010] INFO Created log for partition __consumer_offsets-26 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,011] INFO [Partition __consumer_offsets-26 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,011] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,011] INFO [Partition __consumer_offsets-26 broker=1001] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,021] INFO [Log partition=__consumer_offsets-7, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,023] INFO [Log partition=__consumer_offsets-7, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,024] INFO Created log for partition __consumer_offsets-7 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,024] INFO [Partition __consumer_offsets-7 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,024] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,025] INFO [Partition __consumer_offsets-7 broker=1001] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,045] INFO [Log partition=__consumer_offsets-42, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,046] INFO [Log partition=__consumer_offsets-42, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,047] INFO Created log for partition __consumer_offsets-42 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,047] INFO [Partition __consumer_offsets-42 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,047] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,048] INFO [Partition __consumer_offsets-42 broker=1001] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,057] INFO [Log partition=__consumer_offsets-4, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,061] INFO [Log partition=__consumer_offsets-4, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,063] INFO Created log for partition __consumer_offsets-4 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,064] INFO [Partition __consumer_offsets-4 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,064] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,065] INFO [Partition __consumer_offsets-4 broker=1001] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,073] INFO [Log partition=__consumer_offsets-23, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,074] INFO [Log partition=__consumer_offsets-23, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,077] INFO Created log for partition __consumer_offsets-23 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,078] INFO [Partition __consumer_offsets-23 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,078] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,079] INFO [Partition __consumer_offsets-23 broker=1001] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,088] INFO [Log partition=__consumer_offsets-1, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,089] INFO [Log partition=__consumer_offsets-1, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,091] INFO Created log for partition __consumer_offsets-1 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,091] INFO [Partition __consumer_offsets-1 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,092] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,092] INFO [Partition __consumer_offsets-1 broker=1001] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,101] INFO [Log partition=__consumer_offsets-20, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,102] INFO [Log partition=__consumer_offsets-20, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,103] INFO Created log for partition __consumer_offsets-20 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,103] INFO [Partition __consumer_offsets-20 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,103] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,103] INFO [Partition __consumer_offsets-20 broker=1001] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,112] INFO [Log partition=__consumer_offsets-39, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,112] INFO [Log partition=__consumer_offsets-39, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,114] INFO Created log for partition __consumer_offsets-39 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,114] INFO [Partition __consumer_offsets-39 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,114] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,115] INFO [Partition __consumer_offsets-39 broker=1001] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,124] INFO [Log partition=__consumer_offsets-17, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,125] INFO [Log partition=__consumer_offsets-17, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,126] INFO Created log for partition __consumer_offsets-17 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,126] INFO [Partition __consumer_offsets-17 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,127] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,127] INFO [Partition __consumer_offsets-17 broker=1001] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,139] INFO [Log partition=__consumer_offsets-36, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,140] INFO [Log partition=__consumer_offsets-36, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,141] INFO Created log for partition __consumer_offsets-36 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,142] INFO [Partition __consumer_offsets-36 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,142] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,142] INFO [Partition __consumer_offsets-36 broker=1001] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,163] INFO [Log partition=__consumer_offsets-14, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,165] INFO [Log partition=__consumer_offsets-14, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,169] INFO Created log for partition __consumer_offsets-14 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,170] INFO [Partition __consumer_offsets-14 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,171] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,172] INFO [Partition __consumer_offsets-14 broker=1001] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,204] INFO [Log partition=__consumer_offsets-33, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,208] INFO [Log partition=__consumer_offsets-33, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,212] INFO Created log for partition __consumer_offsets-33 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,214] INFO [Partition __consumer_offsets-33 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,214] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,215] INFO [Partition __consumer_offsets-33 broker=1001] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,249] INFO [Log partition=__consumer_offsets-49, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,251] INFO [Log partition=__consumer_offsets-49, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,253] INFO Created log for partition __consumer_offsets-49 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,254] INFO [Partition __consumer_offsets-49 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,254] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,254] INFO [Partition __consumer_offsets-49 broker=1001] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,271] INFO [Log partition=__consumer_offsets-11, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,272] INFO [Log partition=__consumer_offsets-11, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,273] INFO Created log for partition __consumer_offsets-11 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,274] INFO [Partition __consumer_offsets-11 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,274] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,274] INFO [Partition __consumer_offsets-11 broker=1001] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,285] INFO [Log partition=__consumer_offsets-30, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,286] INFO [Log partition=__consumer_offsets-30, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,287] INFO Created log for partition __consumer_offsets-30 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,288] INFO [Partition __consumer_offsets-30 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,288] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,288] INFO [Partition __consumer_offsets-30 broker=1001] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,298] INFO [Log partition=__consumer_offsets-46, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,300] INFO [Log partition=__consumer_offsets-46, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,301] INFO Created log for partition __consumer_offsets-46 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,302] INFO [Partition __consumer_offsets-46 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,302] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,302] INFO [Partition __consumer_offsets-46 broker=1001] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,312] INFO [Log partition=__consumer_offsets-27, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,314] INFO [Log partition=__consumer_offsets-27, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,315] INFO Created log for partition __consumer_offsets-27 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,318] INFO [Partition __consumer_offsets-27 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,318] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,318] INFO [Partition __consumer_offsets-27 broker=1001] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,329] INFO [Log partition=__consumer_offsets-8, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,330] INFO [Log partition=__consumer_offsets-8, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,331] INFO Created log for partition __consumer_offsets-8 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,332] INFO [Partition __consumer_offsets-8 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,332] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,332] INFO [Partition __consumer_offsets-8 broker=1001] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,341] INFO [Log partition=__consumer_offsets-24, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,342] INFO [Log partition=__consumer_offsets-24, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,343] INFO Created log for partition __consumer_offsets-24 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,344] INFO [Partition __consumer_offsets-24 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,345] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,345] INFO [Partition __consumer_offsets-24 broker=1001] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,358] INFO [Log partition=__consumer_offsets-43, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,359] INFO [Log partition=__consumer_offsets-43, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,360] INFO Created log for partition __consumer_offsets-43 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,361] INFO [Partition __consumer_offsets-43 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,362] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,362] INFO [Partition __consumer_offsets-43 broker=1001] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,379] INFO [Log partition=__consumer_offsets-5, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,380] INFO [Log partition=__consumer_offsets-5, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,382] INFO Created log for partition __consumer_offsets-5 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,382] INFO [Partition __consumer_offsets-5 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,383] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,383] INFO [Partition __consumer_offsets-5 broker=1001] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,400] INFO [Log partition=__consumer_offsets-21, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,401] INFO [Log partition=__consumer_offsets-21, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,402] INFO Created log for partition __consumer_offsets-21 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,403] INFO [Partition __consumer_offsets-21 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,403] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,403] INFO [Partition __consumer_offsets-21 broker=1001] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,417] INFO [Log partition=__consumer_offsets-2, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,417] INFO [Log partition=__consumer_offsets-2, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,419] INFO Created log for partition __consumer_offsets-2 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,419] INFO [Partition __consumer_offsets-2 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,420] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,420] INFO [Partition __consumer_offsets-2 broker=1001] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,429] INFO [Log partition=__consumer_offsets-40, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,430] INFO [Log partition=__consumer_offsets-40, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,431] INFO Created log for partition __consumer_offsets-40 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,432] INFO [Partition __consumer_offsets-40 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,432] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,432] INFO [Partition __consumer_offsets-40 broker=1001] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,441] INFO [Log partition=__consumer_offsets-37, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,442] INFO [Log partition=__consumer_offsets-37, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,443] INFO Created log for partition __consumer_offsets-37 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,444] INFO [Partition __consumer_offsets-37 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,444] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,444] INFO [Partition __consumer_offsets-37 broker=1001] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,457] INFO [Log partition=__consumer_offsets-18, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,458] INFO [Log partition=__consumer_offsets-18, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,459] INFO Created log for partition __consumer_offsets-18 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,460] INFO [Partition __consumer_offsets-18 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,460] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,460] INFO [Partition __consumer_offsets-18 broker=1001] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,470] INFO [Log partition=__consumer_offsets-34, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,471] INFO [Log partition=__consumer_offsets-34, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,473] INFO Created log for partition __consumer_offsets-34 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,474] INFO [Partition __consumer_offsets-34 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,474] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,475] INFO [Partition __consumer_offsets-34 broker=1001] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,484] INFO [Log partition=__consumer_offsets-15, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,485] INFO [Log partition=__consumer_offsets-15, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,486] INFO Created log for partition __consumer_offsets-15 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,486] INFO [Partition __consumer_offsets-15 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,486] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,487] INFO [Partition __consumer_offsets-15 broker=1001] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,496] INFO [Log partition=__consumer_offsets-12, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,497] INFO [Log partition=__consumer_offsets-12, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,498] INFO Created log for partition __consumer_offsets-12 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,498] INFO [Partition __consumer_offsets-12 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,499] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,499] INFO [Partition __consumer_offsets-12 broker=1001] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,509] INFO [Log partition=__consumer_offsets-31, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,509] INFO [Log partition=__consumer_offsets-31, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,510] INFO Created log for partition __consumer_offsets-31 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,511] INFO [Partition __consumer_offsets-31 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,511] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,511] INFO [Partition __consumer_offsets-31 broker=1001] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,520] INFO [Log partition=__consumer_offsets-9, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,522] INFO [Log partition=__consumer_offsets-9, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,523] INFO Created log for partition __consumer_offsets-9 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,525] INFO [Partition __consumer_offsets-9 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,525] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,525] INFO [Partition __consumer_offsets-9 broker=1001] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,548] INFO [Log partition=__consumer_offsets-47, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,549] INFO [Log partition=__consumer_offsets-47, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,550] INFO Created log for partition __consumer_offsets-47 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,551] INFO [Partition __consumer_offsets-47 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,551] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,551] INFO [Partition __consumer_offsets-47 broker=1001] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,562] INFO [Log partition=__consumer_offsets-19, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,563] INFO [Log partition=__consumer_offsets-19, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,565] INFO Created log for partition __consumer_offsets-19 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,565] INFO [Partition __consumer_offsets-19 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,565] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,567] INFO [Partition __consumer_offsets-19 broker=1001] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,579] INFO [Log partition=__consumer_offsets-28, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,581] INFO [Log partition=__consumer_offsets-28, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,584] INFO Created log for partition __consumer_offsets-28 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,586] INFO [Partition __consumer_offsets-28 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,586] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,586] INFO [Partition __consumer_offsets-28 broker=1001] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,602] INFO [Log partition=__consumer_offsets-38, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,603] INFO [Log partition=__consumer_offsets-38, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,604] INFO Created log for partition __consumer_offsets-38 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,605] INFO [Partition __consumer_offsets-38 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,605] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,605] INFO [Partition __consumer_offsets-38 broker=1001] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,614] INFO [Log partition=__consumer_offsets-35, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,615] INFO [Log partition=__consumer_offsets-35, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,616] INFO Created log for partition __consumer_offsets-35 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,617] INFO [Partition __consumer_offsets-35 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,617] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,617] INFO [Partition __consumer_offsets-35 broker=1001] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,634] INFO [Log partition=__consumer_offsets-44, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,635] INFO [Log partition=__consumer_offsets-44, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,636] INFO Created log for partition __consumer_offsets-44 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,637] INFO [Partition __consumer_offsets-44 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,637] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,637] INFO [Partition __consumer_offsets-44 broker=1001] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,647] INFO [Log partition=__consumer_offsets-6, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,648] INFO [Log partition=__consumer_offsets-6, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,650] INFO Created log for partition __consumer_offsets-6 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,651] INFO [Partition __consumer_offsets-6 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,651] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,652] INFO [Partition __consumer_offsets-6 broker=1001] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,669] INFO [Log partition=__consumer_offsets-25, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,670] INFO [Log partition=__consumer_offsets-25, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,671] INFO Created log for partition __consumer_offsets-25 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,672] INFO [Partition __consumer_offsets-25 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,673] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,673] INFO [Partition __consumer_offsets-25 broker=1001] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,682] INFO [Log partition=__consumer_offsets-16, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,682] INFO [Log partition=__consumer_offsets-16, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,683] INFO Created log for partition __consumer_offsets-16 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,684] INFO [Partition __consumer_offsets-16 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,684] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,684] INFO [Partition __consumer_offsets-16 broker=1001] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,694] INFO [Log partition=__consumer_offsets-22, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,695] INFO [Log partition=__consumer_offsets-22, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,696] INFO Created log for partition __consumer_offsets-22 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,696] INFO [Partition __consumer_offsets-22 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,696] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,697] INFO [Partition __consumer_offsets-22 broker=1001] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,715] INFO [Log partition=__consumer_offsets-41, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,717] INFO [Log partition=__consumer_offsets-41, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,719] INFO Created log for partition __consumer_offsets-41 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,720] INFO [Partition __consumer_offsets-41 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,720] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,720] INFO [Partition __consumer_offsets-41 broker=1001] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,730] INFO [Log partition=__consumer_offsets-32, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,732] INFO [Log partition=__consumer_offsets-32, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,734] INFO Created log for partition __consumer_offsets-32 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,735] INFO [Partition __consumer_offsets-32 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,735] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,735] INFO [Partition __consumer_offsets-32 broker=1001] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,750] INFO [Log partition=__consumer_offsets-3, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,751] INFO [Log partition=__consumer_offsets-3, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,753] INFO Created log for partition __consumer_offsets-3 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,753] INFO [Partition __consumer_offsets-3 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,754] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,754] INFO [Partition __consumer_offsets-3 broker=1001] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,764] INFO [Log partition=__consumer_offsets-13, dir=/kafka/kafka-logs-d8ba92357be3] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,765] INFO [Log partition=__consumer_offsets-13, dir=/kafka/kafka-logs-d8ba92357be3] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1               |[0m [2023-03-28 11:13:52,766] INFO Created log for partition __consumer_offsets-13 in /kafka/kafka-logs-d8ba92357be3 with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,766] INFO [Partition __consumer_offsets-13 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,766] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1               |[0m [2023-03-28 11:13:52,767] INFO [Partition __consumer_offsets-13 broker=1001] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1               |[0m [2023-03-28 11:13:52,774] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,776] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,777] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,777] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,777] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,777] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,778] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,778] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,778] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,778] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,778] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,779] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,779] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,779] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,779] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,779] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,779] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,780] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,780] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,780] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,780] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,780] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,780] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,781] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,782] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,782] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,782] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,782] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,782] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,782] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,782] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,783] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,783] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,783] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,783] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,783] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,783] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,796] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-22 in 20 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,802] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,802] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,806] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-31 in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,807] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,807] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,808] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,808] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,809] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-46 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,809] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,810] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,811] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-44 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,812] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,813] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-1 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,813] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,813] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,813] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,814] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,814] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,814] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,814] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,814] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,815] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,815] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,815] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,815] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,816] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,816] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,816] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,816] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,817] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,817] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,818] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,819] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,819] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,820] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,821] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,821] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,822] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,822] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,822] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,823] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,824] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-27 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,824] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,825] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,825] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,826] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-39 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,826] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,827] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:52,827] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1               |[0m [2023-03-28 11:13:53,014] INFO [GroupCoordinator 1001]: Preparing to rebalance group console-consumer-75520 in state PreparingRebalance with old generation 0 (__consumer_offsets-10) (reason: Adding new member consumer-1-889bd406-4d32-48be-b35f-058815fdf667) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:13:53,051] INFO [GroupCoordinator 1001]: Stabilized group console-consumer-75520 generation 1 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:13:53,074] INFO [GroupCoordinator 1001]: Assignment received from leader for group console-consumer-75520 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:16:03,568] INFO [GroupCoordinator 1001]: Preparing to rebalance group console-consumer-92978 in state PreparingRebalance with old generation 0 (__consumer_offsets-36) (reason: Adding new member consumer-1-8230a8ae-bd8e-4356-82ac-15a6e8b3f762) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:16:03,570] INFO [GroupCoordinator 1001]: Stabilized group console-consumer-92978 generation 1 (__consumer_offsets-36) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:16:03,575] INFO [GroupCoordinator 1001]: Assignment received from leader for group console-consumer-92978 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,250 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job bfe74f5a3dd0e511ceff6853162ed9dd. Free slots: 0
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,258 WARN  org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge [] - Could not acquire the minimum required resources, failing slot requests. Acquired: [ResourceRequirement{resourceProfile=ResourceProfile{cpuCores=1, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, numberOfRequiredSlots=1}]. Current slot pool status: Registered TMs: 1, registered slots: 1 free slots: 0
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,291 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: incoming-events -> Sink: event-log (2/2) (4c445d5e62cfe4f736eed1555839d7c5) switched from SCHEDULED to FAILED on [unassigned resource].
[33mjobmanager_1          |[0m org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,346 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 4c445d5e62cfe4f736eed1555839d7c5.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,363 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_1.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,369 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 4 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_1. 
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,380 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Fraud Detection (bfe74f5a3dd0e511ceff6853162ed9dd) switched from state RUNNING to FAILING.
[33mjobmanager_1          |[0m org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1473) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1133) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1073) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:912) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:474) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:609) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignAllResourcesAndRegisterProducedPartitions$6(DefaultScheduler.java:481) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequest.failRequest(DeclarativeSlotPoolBridge.java:545) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:127) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:355) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:344) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:809) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:308) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_345]
[33mjobmanager_1          |[0m Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResource$8(DefaultScheduler.java:539) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	... 39 more
[33mjobmanager_1          |[0m Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	... 37 more
[33mjobmanager_1          |[0m Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,413 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: incoming-events -> Sink: event-log (1/2) (67082fafc4ef711d379774e220eb3a77) switched from SCHEDULED to CANCELING.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,435 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: incoming-events -> Sink: event-log (1/2) (67082fafc4ef711d379774e220eb3a77) switched from CANCELING to CANCELED.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,439 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 67082fafc4ef711d379774e220eb3a77.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,452 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 67082fafc4ef711d379774e220eb3a77.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,457 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - fraud-detector -> Sink: send-alerts (1/2) (5204fc218efe3c04aab7880bcf98e43c) switched from SCHEDULED to CANCELING.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,460 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - fraud-detector -> Sink: send-alerts (1/2) (5204fc218efe3c04aab7880bcf98e43c) switched from CANCELING to CANCELED.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,464 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 5204fc218efe3c04aab7880bcf98e43c.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,464 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - fraud-detector -> Sink: send-alerts (2/2) (b39819125cf6b9d802a155ce8a4f54a9) switched from SCHEDULED to CANCELING.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,464 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job bfe74f5a3dd0e511ceff6853162ed9dd: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,465 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - fraud-detector -> Sink: send-alerts (2/2) (b39819125cf6b9d802a155ce8a4f54a9) switched from CANCELING to CANCELED.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,467 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b39819125cf6b9d802a155ce8a4f54a9.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,469 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Fraud Detection (bfe74f5a3dd0e511ceff6853162ed9dd) switched from state FAILING to FAILED.
[33mjobmanager_1          |[0m org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1473) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1133) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1073) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:912) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:474) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:609) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignAllResourcesAndRegisterProducedPartitions$6(DefaultScheduler.java:481) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequest.failRequest(DeclarativeSlotPoolBridge.java:545) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:127) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:355) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:344) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:809) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:308) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_93935039-830a-4cd1-89ac-71a003a44ebb.jar:1.14.6]
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_345]
[33mjobmanager_1          |[0m Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResource$8(DefaultScheduler.java:539) ~[flink-dist_2.12-1.14.6.jar:1.14.6]
[33mjobmanager_1          |[0m 	... 39 more
[33mjobmanager_1          |[0m Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_345]
[33mjobmanager_1          |[0m 	... 37 more
[33mjobmanager_1          |[0m Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,475 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job bfe74f5a3dd0e511ceff6853162ed9dd.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,497 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job bfe74f5a3dd0e511ceff6853162ed9dd
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,504 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job bfe74f5a3dd0e511ceff6853162ed9dd reached terminal state FAILED.
[33mjobmanager_1          |[0m org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1473)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1133)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1073)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:912)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:474)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:609)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignAllResourcesAndRegisterProducedPartitions$6(DefaultScheduler.java:481)
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequest.failRequest(DeclarativeSlotPoolBridge.java:545)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:127)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:355)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:344)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:809)
[33mjobmanager_1          |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[33mjobmanager_1          |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[33mjobmanager_1          |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[33mjobmanager_1          |[0m 	at java.lang.reflect.Method.invoke(Method.java:498)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:308)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
[33mjobmanager_1          |[0m 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
[33mjobmanager_1          |[0m 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
[33mjobmanager_1          |[0m 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
[33mjobmanager_1          |[0m 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
[33mjobmanager_1          |[0m 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
[33mjobmanager_1          |[0m 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
[33mjobmanager_1          |[0m 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
[33mjobmanager_1          |[0m 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
[33mjobmanager_1          |[0m 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
[33mjobmanager_1          |[0m 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
[33mjobmanager_1          |[0m 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
[33mjobmanager_1          |[0m 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
[33mjobmanager_1          |[0m 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
[33mjobmanager_1          |[0m 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
[33mjobmanager_1          |[0m 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
[33mjobmanager_1          |[0m 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
[33mjobmanager_1          |[0m 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
[33mjobmanager_1          |[0m Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
[33mjobmanager_1          |[0m 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResource$8(DefaultScheduler.java:539)
[33mjobmanager_1          |[0m 	... 39 more
[33mjobmanager_1          |[0m Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
[33mjobmanager_1          |[0m 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
[33mjobmanager_1          |[0m 	... 37 more
[33mjobmanager_1          |[0m Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,592 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'Fraud Detection' (bfe74f5a3dd0e511ceff6853162ed9dd).
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,613 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,617 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [cf9088bc502bdce73989248dde6339d2].
[36;1mtaskmanager_1         |[0m 2023-03-28 11:17:42,624 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: cf9088bc502bdce73989248dde6339d2, jobId: bfe74f5a3dd0e511ceff6853162ed9dd).
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,630 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection fe74036af6d8d36dfd28422f566367ef: Stopping JobMaster for job 'Fraud Detection' (bfe74f5a3dd0e511ceff6853162ed9dd).
[36;1mtaskmanager_1         |[0m 2023-03-28 11:17:42,632 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job bfe74f5a3dd0e511ceff6853162ed9dd from job leader monitoring.
[36;1mtaskmanager_1         |[0m 2023-03-28 11:17:42,633 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job bfe74f5a3dd0e511ceff6853162ed9dd.
[33mjobmanager_1          |[0m 2023-03-28 11:17:42,633 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@jobmanager:6123/user/rpc/jobmanager_2 for job bfe74f5a3dd0e511ceff6853162ed9dd from the resource manager.
[32mkafka_1               |[0m [2023-03-28 11:17:45,072] INFO [GroupCoordinator 1001]: Preparing to rebalance group console-consumer-65501 in state PreparingRebalance with old generation 0 (__consumer_offsets-28) (reason: Adding new member consumer-1-b2db0853-00ff-4934-9696-52342dec3f85) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:17:45,073] INFO [GroupCoordinator 1001]: Stabilized group console-consumer-65501 generation 1 (__consumer_offsets-28) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:17:45,079] INFO [GroupCoordinator 1001]: Assignment received from leader for group console-consumer-65501 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:17:53,994] INFO [GroupCoordinator 1001]: Preparing to rebalance group console-consumer-67247 in state PreparingRebalance with old generation 0 (__consumer_offsets-7) (reason: Adding new member consumer-1-a5b05322-2d59-4205-b559-c49fe7f78188) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:17:53,996] INFO [GroupCoordinator 1001]: Stabilized group console-consumer-67247 generation 1 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:17:54,002] INFO [GroupCoordinator 1001]: Assignment received from leader for group console-consumer-67247 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:18:05,757] INFO [GroupCoordinator 1001]: Preparing to rebalance group console-consumer-81277 in state PreparingRebalance with old generation 0 (__consumer_offsets-46) (reason: Adding new member consumer-1-e00db5fc-0ded-47fe-ad16-be642c05f9a5) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:18:05,758] INFO [GroupCoordinator 1001]: Stabilized group console-consumer-81277 generation 1 (__consumer_offsets-46) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:18:05,780] INFO [GroupCoordinator 1001]: Assignment received from leader for group console-consumer-81277 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:18:24,490] INFO [GroupCoordinator 1001]: Preparing to rebalance group console-consumer-14369 in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member consumer-1-535e3361-a49f-4371-bbf0-3509a33d1449) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:18:24,494] INFO [GroupCoordinator 1001]: Stabilized group console-consumer-14369 generation 1 (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:18:24,502] INFO [GroupCoordinator 1001]: Assignment received from leader for group console-consumer-14369 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1               |[0m [2023-03-28 11:22:38,981] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 34 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
Gracefully stopping... (press Ctrl+C again to force)
